#!/usr/bin/env python3
"""
å‘é‡æ£€ç´¢å™¨
å°è£… FAISS ç´¢å¼•åŠ è½½å’Œç›¸ä¼¼åº¦æœç´¢åŠŸèƒ½
"""

import json
import os
import logging
import hashlib
import time
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timedelta
from functools import lru_cache
import faiss
import numpy as np
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# é…ç½®å¸¸é‡
API_BASE_URL = "https://space.ai-builders.com/backend"
EMBEDDING_MODEL = "text-embedding-3-small"
MAX_RETRIES = 3
EMBEDDING_CACHE_SIZE = 1000  # LRU ç¼“å­˜å¤§å°

# åˆ›å»ºæ—¥å¿—å™¨
logger = logging.getLogger("vector_indexer.retriever")


class EmbeddingClient:
    """å°è£…embeddings APIè°ƒç”¨çš„å®¢æˆ·ç«¯ï¼Œå¸¦ç¼“å­˜åŠŸèƒ½"""
    
    def __init__(self, api_key: str, base_url: str = API_BASE_URL, cache_size: int = EMBEDDING_CACHE_SIZE):
        self.api_key = api_key
        self.base_url = base_url
        self.session = self._create_session()
        self.cache_size = cache_size
        self._cache: Dict[str, List[float]] = {}
        self._cache_hits = 0
        self._cache_misses = 0
    
    def _create_session(self):
        """åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„requests session"""
        session = requests.Session()
        retry_strategy = Retry(
            total=MAX_RETRIES,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["POST"]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("https://", adapter)
        return session
    
    def _get_cache_key(self, text: str, model: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_string = f"{model}:{text}"
        return hashlib.md5(key_string.encode('utf-8')).hexdigest()
    
    def get_embedding(self, text: str, model: str = EMBEDDING_MODEL) -> List[float]:
        """
        è·å–å•ä¸ªæ–‡æœ¬çš„embeddingï¼ˆå¸¦ç¼“å­˜ï¼‰
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            model: æ¨¡å‹åç§°
            
        Returns:
            embeddingå‘é‡
        """
        cache_key = self._get_cache_key(text, model)
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self._cache:
            self._cache_hits += 1
            logger.debug(f"ğŸ’¾ ç¼“å­˜å‘½ä¸­: text_length={len(text)}")
            return self._cache[cache_key]
        
        # ç¼“å­˜æœªå‘½ä¸­ï¼Œè°ƒç”¨ API
        self._cache_misses += 1
        url = f"{self.base_url}/v1/embeddings"
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "input": [text],
            "model": model
        }
        
        try:
            logger.debug(f"ğŸ”— è°ƒç”¨ Embedding API: model={model}, text_length={len(text)}")
            response = self.session.post(url, json=payload, headers=headers, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            embedding = result["data"][0]["embedding"]
            logger.debug(f"âœ… Embedding ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(embedding)}")
            
            # å­˜å…¥ç¼“å­˜ï¼ˆå¦‚æœç¼“å­˜å·²æ»¡ï¼Œåˆ é™¤æœ€æ—§çš„é¡¹ï¼‰
            if len(self._cache) >= self.cache_size:
                # åˆ é™¤ç¬¬ä¸€ä¸ªï¼ˆæœ€æ—§çš„ï¼‰é¡¹
                oldest_key = next(iter(self._cache))
                del self._cache[oldest_key]
            
            self._cache[cache_key] = embedding
            return embedding
        
        except requests.exceptions.RequestException as e:
            logger.exception(f"âŒ Embedding API è°ƒç”¨å¤±è´¥: {e}")
            raise Exception(f"APIè°ƒç”¨å¤±è´¥: {e}")
    
    def get_cache_stats(self) -> Dict[str, int]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total = self._cache_hits + self._cache_misses
        hit_rate = (self._cache_hits / total * 100) if total > 0 else 0.0
        return {
            "cache_size": len(self._cache),
            "max_cache_size": self.cache_size,
            "cache_hits": self._cache_hits,
            "cache_misses": self._cache_misses,
            "hit_rate": round(hit_rate, 2)
        }
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self._cache.clear()
        self._cache_hits = 0
        self._cache_misses = 0
        logger.info("ğŸ—‘ï¸  Embedding ç¼“å­˜å·²æ¸…ç©º")


class VectorRetriever:
    """å‘é‡æ£€ç´¢å™¨ç±»"""
    
    def __init__(self, index_path: str, metadata_path: str, api_key: str, enable_cache: bool = True):
        """
        åˆå§‹åŒ–æ£€ç´¢å™¨
        
        Args:
            index_path: FAISSç´¢å¼•æ–‡ä»¶è·¯å¾„
            metadata_path: å…ƒæ•°æ®JSONæ–‡ä»¶è·¯å¾„
            api_key: AI Builder APIå¯†é’¥
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
        """
        self.index_path = index_path
        self.metadata_path = metadata_path
        self.index = None
        self.metadata = None
        self.enable_cache = enable_cache
        self.embedding_client = EmbeddingClient(api_key)
        self._load_index_and_metadata()
    
    def _load_index_and_metadata(self):
        """åŠ è½½FAISSç´¢å¼•å’Œå…ƒæ•°æ®"""
        logger.info(f"ğŸ“– æ­£åœ¨åŠ è½½ç´¢å¼•: {self.index_path}")
        
        if not os.path.exists(self.index_path):
            logger.error(f"âŒ ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {self.index_path}")
            raise FileNotFoundError(f"ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {self.index_path}")
        
        if not os.path.exists(self.metadata_path):
            logger.error(f"âŒ å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.metadata_path}")
            raise FileNotFoundError(f"å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.metadata_path}")
        
        # åŠ è½½FAISSç´¢å¼•
        try:
            self.index = faiss.read_index(self.index_path)
            logger.info(f"âœ… ç´¢å¼•åŠ è½½å®Œæˆï¼Œå‘é‡æ•°é‡: {self.index.ntotal}, ç»´åº¦: {self.index.d}")
        except Exception as e:
            logger.exception(f"âŒ ç´¢å¼•åŠ è½½å¤±è´¥: {e}")
            raise
        
        # åŠ è½½å…ƒæ•°æ®
        logger.info(f"ğŸ“– æ­£åœ¨åŠ è½½å…ƒæ•°æ®: {self.metadata_path}")
        try:
            with open(self.metadata_path, 'r', encoding='utf-8') as f:
                self.metadata = json.load(f)
            logger.info(f"âœ… å…ƒæ•°æ®åŠ è½½å®Œæˆï¼Œè®°å½•æ•°é‡: {len(self.metadata)}")
        except Exception as e:
            logger.exception(f"âŒ å…ƒæ•°æ®åŠ è½½å¤±è´¥: {e}")
            raise
        
        # éªŒè¯ç´¢å¼•å’Œå…ƒæ•°æ®æ•°é‡æ˜¯å¦ä¸€è‡´
        if self.index.ntotal != len(self.metadata):
            error_msg = f"ç´¢å¼•å‘é‡æ•°é‡ ({self.index.ntotal}) ä¸å…ƒæ•°æ®æ•°é‡ ({len(self.metadata)}) ä¸åŒ¹é…"
            logger.error(f"âŒ {error_msg}")
            raise ValueError(error_msg)
    
    def _parse_date_filter(self, date_filter: Optional[str], current_date: datetime) -> Optional[Tuple[datetime, datetime]]:
        """
        è§£ææ—¥æœŸè¿‡æ»¤æ¡ä»¶
        
        Args:
            date_filter: æ—¥æœŸè¿‡æ»¤å­—ç¬¦ä¸²ï¼Œæ”¯æŒï¼š
                - "YYYY-MM-DD" æ ¼å¼çš„å…·ä½“æ—¥æœŸ
                - "YYYY-MM" æ ¼å¼çš„å¹´æœˆï¼ˆå¦‚ "2024-03"ï¼‰
                - "YYYY" æ ¼å¼çš„å¹´ä»½ï¼ˆå¦‚ "2024"ï¼‰
                - "last_year", "last_month", "last_week" ç­‰ç›¸å¯¹æ—¶é—´
                - "N_months_ago" æ ¼å¼ï¼ˆå¦‚ "3_months_ago"ï¼‰
                - "N_days_ago" æ ¼å¼ï¼ˆå¦‚ "30_days_ago"ï¼‰
            current_date: å½“å‰æ—¥æœŸ
            
        Returns:
            (start_date, end_date) å…ƒç»„ï¼Œå¦‚æœä¸ºNoneåˆ™è¡¨ç¤ºä¸è¿‡æ»¤
        """
        if not date_filter:
            return None
        
        date_filter = date_filter.strip().lower()
        
        # å¤„ç†ç›¸å¯¹æ—¶é—´
        if date_filter == "last_year":
            start_date = current_date.replace(year=current_date.year - 1, month=1, day=1)
            end_date = current_date.replace(year=current_date.year - 1, month=12, day=31)
            return (start_date, end_date)
        
        elif date_filter == "last_month":
            if current_date.month == 1:
                start_date = current_date.replace(year=current_date.year - 1, month=12, day=1)
            else:
                start_date = current_date.replace(month=current_date.month - 1, day=1)
            
            # è®¡ç®—ä¸Šä¸ªæœˆçš„æœ€åä¸€å¤©
            if current_date.month == 1:
                end_date = current_date.replace(year=current_date.year - 1, month=12, day=31)
            else:
                # è·å–ä¸Šä¸ªæœˆçš„æœ€åä¸€å¤©
                first_day_this_month = current_date.replace(day=1)
                last_day_last_month = first_day_this_month - timedelta(days=1)
                end_date = last_day_last_month
            
            return (start_date, end_date)
        
        elif date_filter == "last_week":
            end_date = current_date - timedelta(days=current_date.weekday() + 1)  # ä¸Šå‘¨æ—¥
            start_date = end_date - timedelta(days=6)  # ä¸Šå‘¨ä¸€
            return (start_date, end_date)
        
        # å¤„ç† "N_months_ago" æ ¼å¼ï¼ˆå¦‚ "3_months_ago"ï¼‰
        if date_filter.endswith("_months_ago"):
            try:
                months = int(date_filter.replace("_months_ago", ""))
                # è®¡ç®— N ä¸ªæœˆå‰çš„æ—¥æœŸèŒƒå›´
                end_date = current_date - timedelta(days=1)  # æ˜¨å¤©
                # è®¡ç®— N ä¸ªæœˆå‰çš„æ—¥æœŸ
                start_date = current_date
                for _ in range(months):
                    if start_date.month == 1:
                        start_date = start_date.replace(year=start_date.year - 1, month=12)
                    else:
                        start_date = start_date.replace(month=start_date.month - 1)
                start_date = start_date.replace(day=1)  # æœˆåˆ
                return (start_date, end_date)
            except ValueError:
                pass
        
        # å¤„ç† "N_days_ago" æ ¼å¼ï¼ˆå¦‚ "30_days_ago"ï¼‰
        # æ³¨æ„ï¼š"N_days_ago" è¡¨ç¤º"æœ€è¿‘Nå¤©"ï¼ŒåŒ…æ‹¬ä»Šå¤©
        # ä¾‹å¦‚ï¼š"2_days_ago" è¡¨ç¤ºæœ€è¿‘2å¤©ï¼Œå³æ˜¨å¤©å’Œä»Šå¤©
        if date_filter.endswith("_days_ago"):
            try:
                days = int(date_filter.replace("_days_ago", ""))
                end_date = current_date  # åŒ…æ‹¬ä»Šå¤©
                start_date = current_date - timedelta(days=days - 1)  # ä» N-1 å¤©å‰å¼€å§‹ï¼ˆåŒ…æ‹¬ä»Šå¤©ï¼‰
                return (start_date, end_date)
            except ValueError:
                pass
        
        # å¤„ç† "YYYY-MM-DD" æ ¼å¼çš„å…·ä½“æ—¥æœŸ
        try:
            filter_date = datetime.strptime(date_filter, "%Y-%m-%d")
            return (filter_date, filter_date)
        except ValueError:
            pass
        
        # å¤„ç† "YYYY-MM" æ ¼å¼çš„å¹´æœˆï¼ˆå¦‚ "2024-03"ï¼‰
        try:
            parts = date_filter.split("-")
            if len(parts) == 2:
                year, month = parts
                year = int(year)
                month = int(month)
                if 1 <= month <= 12:
                    start_date = datetime(year, month, 1)
                    # è®¡ç®—è¯¥æœˆçš„æœ€åä¸€å¤©
                    if month == 12:
                        end_date = datetime(year, 12, 31)
                    else:
                        next_month = datetime(year, month + 1, 1)
                        end_date = next_month - timedelta(days=1)
                    return (start_date, end_date)
        except (ValueError, AttributeError):
            pass
        
        # å¤„ç† "YYYY-MM-ä¸‹æ—¬" æˆ– "YYYY-MM-ä¸Šæ—¬" æˆ– "YYYY-MM-ä¸­æ—¬" æ ¼å¼
        # æ³¨æ„ï¼šè¿™é‡Œ date_filter å¯èƒ½æ˜¯ "2024-11-ä¸‹æ—¬" è¿™æ ·çš„æ ¼å¼
        if "ä¸‹æ—¬" in date_filter or "ä¸Šæ—¬" in date_filter or "ä¸­æ—¬" in date_filter:
            try:
                # æå–å¹´æœˆ
                parts = date_filter.replace("ä¸‹æ—¬", "").replace("ä¸Šæ—¬", "").replace("ä¸­æ—¬", "").split("-")
                if len(parts) >= 2:
                    year = int(parts[0])
                    month = int(parts[1])
                    if 1 <= month <= 12:
                        if "ä¸Šæ—¬" in date_filter:
                            # ä¸Šæ—¬ï¼š1-10æ—¥
                            start_date = datetime(year, month, 1)
                            end_date = datetime(year, month, 10)
                        elif "ä¸­æ—¬" in date_filter:
                            # ä¸­æ—¬ï¼š11-20æ—¥
                            start_date = datetime(year, month, 11)
                            end_date = datetime(year, month, 20)
                        elif "ä¸‹æ—¬" in date_filter:
                            # ä¸‹æ—¬ï¼š21æ—¥-æœˆæœ«
                            start_date = datetime(year, month, 21)
                            # è®¡ç®—è¯¥æœˆçš„æœ€åä¸€å¤©
                            if month == 12:
                                end_date = datetime(year, 12, 31)
                            else:
                                next_month = datetime(year, month + 1, 1)
                                end_date = next_month - timedelta(days=1)
                        return (start_date, end_date)
            except (ValueError, AttributeError):
                pass
        
        # å¤„ç† "YYYY" æ ¼å¼çš„å¹´ä»½ï¼ˆå¦‚ "2024"ï¼‰
        try:
            year = int(date_filter)
            if 1900 <= year <= 2100:  # åˆç†çš„å¹´ä»½èŒƒå›´
                start_date = datetime(year, 1, 1)
                end_date = datetime(year, 12, 31)
                return (start_date, end_date)
        except ValueError:
            pass
        
        # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›Noneï¼ˆä¸è¿‡æ»¤ï¼‰
        logger.warning(f"âš ï¸  æ— æ³•è§£ææ—¥æœŸè¿‡æ»¤æ¡ä»¶: {date_filter}ï¼Œå°†å¿½ç•¥æ—¥æœŸè¿‡æ»¤")
        return None
    
    def _filter_by_date(self, indices: List[int], date_range: Optional[Tuple[datetime, datetime]]) -> List[int]:
        """
        æ ¹æ®æ—¥æœŸèŒƒå›´è¿‡æ»¤ç»“æœ
        
        Args:
            indices: åŸå§‹ç´¢å¼•åˆ—è¡¨
            date_range: (start_date, end_date) å…ƒç»„
            
        Returns:
            è¿‡æ»¤åçš„ç´¢å¼•åˆ—è¡¨
        """
        if not date_range:
            return indices
        
        start_date, end_date = date_range
        filtered_indices = []
        
        for idx in indices:
            chunk = self.metadata[idx]
            chunk_date = chunk.get('date')
            
            # å¦‚æœchunkæ²¡æœ‰æ—¥æœŸï¼Œè·³è¿‡ï¼ˆä¸åŒ…å«åœ¨ç»“æœä¸­ï¼‰
            if not chunk_date:
                continue
            
            try:
                # è§£ææ—¥æœŸå­—ç¬¦ä¸²
                if isinstance(chunk_date, str):
                    chunk_dt = datetime.strptime(chunk_date, "%Y-%m-%d")
                else:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦åœ¨æ—¥æœŸèŒƒå›´å†…
                if start_date <= chunk_dt <= end_date:
                    filtered_indices.append(idx)
            
            except (ValueError, TypeError):
                # æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè·³è¿‡
                continue
        
        return filtered_indices
    
    def search(
        self,
        query: str,
        top_k: int = 5,
        date_filter: Optional[str] = None,
        current_date: Optional[datetime] = None
    ) -> List[Dict[str, Any]]:
        """
        æœç´¢ç›¸ä¼¼å†…å®¹
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›å‰Kä¸ªç»“æœ
            date_filter: å¯é€‰çš„æ—¥æœŸè¿‡æ»¤æ¡ä»¶
            current_date: å½“å‰æ—¥æœŸï¼ˆç”¨äºè§£æç›¸å¯¹æ—¶é—´ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰æ—¶é—´
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
                - id: chunk ID
                - source: æ¥æº
                - date: æ—¥æœŸ
                - content: å†…å®¹
                - distance: è·ç¦»ï¼ˆè¶Šå°è¶Šç›¸ä¼¼ï¼‰
        """
        if current_date is None:
            current_date = datetime.now()
        
        logger.debug(f"ğŸ” å¼€å§‹æœç´¢: query='{query[:50]}...', top_k={top_k}, date_filter={date_filter}")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # 1. ç”ŸæˆæŸ¥è¯¢æ–‡æœ¬çš„embedding
        try:
            embedding_start = time.time()
            query_embedding = self.embedding_client.get_embedding(query)
            query_vector = np.array([query_embedding], dtype=np.float32)
            embedding_time = time.time() - embedding_start
            logger.debug(f"âœ… æŸ¥è¯¢å‘é‡ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(query_embedding)}ï¼Œè€—æ—¶: {embedding_time:.2f}ç§’")
        except Exception as e:
            logger.exception(f"âŒ ç”ŸæˆæŸ¥è¯¢å‘é‡å¤±è´¥: {e}")
            raise
        
        # 2. æœç´¢ç­–ç•¥
        # å¦‚æœè®¾ç½®äº†æ—¥æœŸè¿‡æ»¤ï¼Œéœ€è¦æœç´¢æ›´å¤šå€™é€‰ç»“æœï¼ˆå› ä¸ºè¿‡æ»¤åå¯èƒ½ç»“æœä¸è¶³ï¼‰
        if date_filter:
            # æ—¥æœŸè¿‡æ»¤æ—¶ï¼Œæœç´¢æ›´å¤šç»“æœä»¥ç¡®ä¿æœ‰è¶³å¤Ÿçš„å€™é€‰
            # å¯¹äºå…·ä½“æ—¥æœŸï¼ˆå¦‚ "2024-06-02"ï¼‰ï¼Œæœç´¢æ‰€æœ‰ç»“æœç„¶åè¿‡æ»¤
            # å¯¹äºç›¸å¯¹æ—¶é—´ï¼ˆå¦‚ "last_year"ï¼‰æˆ–æ—¥æœŸèŒƒå›´ï¼ˆå¦‚ "2024-11-ä¸‹æ—¬"ï¼‰ï¼Œæœç´¢æ›´å¤šç»“æœ
            date_filter_clean = date_filter.strip()
            if len(date_filter_clean) == 10 and date_filter_clean.count('-') == 2 and date_filter_clean.count('ä¸‹æ—¬') == 0:
                # å…·ä½“æ—¥æœŸæ ¼å¼ YYYY-MM-DDï¼ˆä¸å«"ä¸‹æ—¬"ç­‰ï¼‰
                # ä¼˜åŒ–ï¼šä¸éœ€è¦æœç´¢æ‰€æœ‰ç»“æœï¼Œæœç´¢åˆç†æ•°é‡å³å¯ï¼ˆtop_k * 200 åº”è¯¥è¶³å¤Ÿï¼‰
                # è¿™æ ·å¯ä»¥é¿å…åœ¨å¤§ç´¢å¼•ä¸Šæœç´¢æ‰€æœ‰ç»“æœå¯¼è‡´çš„æ€§èƒ½é—®é¢˜
                search_k = min(top_k * 200, self.index.ntotal)
                logger.debug(f"ğŸ“… å…·ä½“æ—¥æœŸæŸ¥è¯¢ï¼Œæœç´¢ {search_k} æ¡å€™é€‰ç»“æœï¼ˆç´¢å¼•æ€»æ•°: {self.index.ntotal}ï¼‰")
            elif "ä¸‹æ—¬" in date_filter_clean or "ä¸Šæ—¬" in date_filter_clean or "ä¸­æ—¬" in date_filter_clean:
                # æ—¥æœŸèŒƒå›´ï¼ˆå¦‚ "2024-11-ä¸‹æ—¬"ï¼‰ï¼Œæœç´¢æ›´å¤šç»“æœä»¥ç¡®ä¿è¦†ç›–æ•´ä¸ªèŒƒå›´
                search_k = min(top_k * 100, self.index.ntotal)  # å¢åŠ æœç´¢æ•°é‡
            else:
                # ç›¸å¯¹æ—¶é—´ï¼Œæœç´¢æ›´å¤šç»“æœ
                search_k = min(top_k * 50, self.index.ntotal)
        else:
            search_k = top_k
        
        # 2. æ‰§è¡ŒFAISSæœç´¢
        search_start = time.time()
        distances, indices = self.index.search(query_vector, search_k)
        search_time = time.time() - search_start
        logger.debug(f"ğŸ” FAISSæœç´¢å®Œæˆï¼Œæœç´¢äº† {search_k} æ¡ï¼Œè€—æ—¶: {search_time:.2f}ç§’")
        
        # 3. åº”ç”¨æ—¥æœŸè¿‡æ»¤ï¼ˆå¦‚æœæœ‰ï¼‰
        if date_filter:
            try:
                date_range = self._parse_date_filter(date_filter, current_date)
                logger.debug(f"ğŸ“… æ—¥æœŸè¿‡æ»¤: {date_filter} -> {date_range}")
                if date_range is None:
                    # æ—¥æœŸè§£æå¤±è´¥ï¼Œè®°å½•è­¦å‘Šä½†ç»§ç»­æœç´¢ï¼ˆä¸è¿‡æ»¤ï¼‰
                    logger.warning(f"âš ï¸  æ— æ³•è§£ææ—¥æœŸè¿‡æ»¤æ¡ä»¶ '{date_filter}'ï¼Œå°†å¿½ç•¥æ—¥æœŸè¿‡æ»¤")
                    result_indices = indices[0].tolist()[:top_k]
                else:
                    filtered_indices = self._filter_by_date(indices[0].tolist(), date_range)
                    logger.debug(f"ğŸ“Š æ—¥æœŸè¿‡æ»¤ç»“æœ: åŸå§‹ {len(indices[0])} æ¡ -> è¿‡æ»¤å {len(filtered_indices)} æ¡")
                    # è¿”å›è¿‡æ»¤åçš„ç»“æœï¼ˆæŒ‰ç›¸ä¼¼åº¦æ’åºï¼‰
                    result_indices = filtered_indices[:top_k]
            except Exception as e:
                # æ—¥æœŸè§£æå¼‚å¸¸ï¼Œè®°å½•é”™è¯¯ä½†ç»§ç»­æœç´¢ï¼ˆä¸è¿‡æ»¤ï¼‰
                logger.exception(f"âš ï¸  æ—¥æœŸè¿‡æ»¤å¤„ç†å¼‚å¸¸: {e}ï¼Œå°†å¿½ç•¥æ—¥æœŸè¿‡æ»¤")
                result_indices = indices[0].tolist()[:top_k]
        else:
            result_indices = indices[0].tolist()[:top_k]
        
        # 4. æ„å»ºç»“æœå¹¶å»é‡
        seen_ids = set()  # ç”¨äºå»é‡
        results = []
        
        # åˆ›å»ºç´¢å¼•åˆ°è·ç¦»çš„æ˜ å°„ï¼Œæé«˜æŸ¥æ‰¾æ•ˆç‡å¹¶é¿å…å¼‚å¸¸
        idx_to_distance = {}
        for i, idx in enumerate(indices[0].tolist()):
            idx_to_distance[idx] = float(distances[0][i])
        
        for idx in result_indices:
            try:
                chunk = self.metadata[idx]
                chunk_id = chunk.get("id")
                
                # å»é‡ï¼šå¦‚æœå·²ç»è§è¿‡è¿™ä¸ª IDï¼Œè·³è¿‡
                if chunk_id and chunk_id in seen_ids:
                    continue
                
                seen_ids.add(chunk_id)
                
                # æ‰¾åˆ°å¯¹åº”çš„è·ç¦»ï¼ˆä½¿ç”¨æ˜ å°„ï¼Œé¿å… index() å¯èƒ½æŠ›å‡ºçš„å¼‚å¸¸ï¼‰
                distance = idx_to_distance.get(idx, 1.0)  # å¦‚æœæ‰¾ä¸åˆ°ï¼Œä½¿ç”¨é»˜è®¤å€¼1.0
                
                results.append({
                    "id": chunk_id,
                    "source": chunk.get("source"),
                    "date": chunk.get("date"),
                    "content": chunk.get("content"),
                    "distance": distance
                })
            except (IndexError, KeyError, TypeError) as e:
                # å¦‚æœç´¢å¼•æ— æ•ˆæˆ–å…ƒæ•°æ®æœ‰é—®é¢˜ï¼Œè·³è¿‡è¿™æ¡è®°å½•
                logger.warning(f"âš ï¸  è·³è¿‡æ— æ•ˆç´¢å¼• {idx}: {e}")
                continue
        
        # 5. æŒ‰è·ç¦»æ’åºï¼ˆç¡®ä¿ç»“æœæŒ‰ç›¸ä¼¼åº¦ä»é«˜åˆ°ä½ï¼‰
        results.sort(key=lambda x: x["distance"])
        
        # å¦‚æœå»é‡åç»“æœä¸è¶³ï¼Œå°è¯•è¡¥å……
        if len(results) < top_k and len(result_indices) < len(indices[0]):
            # ä»å‰©ä½™çš„å€™é€‰ä¸­è¡¥å……
            remaining_indices = [idx for idx in indices[0].tolist() if idx not in result_indices]
            for idx in remaining_indices[:top_k - len(results)]:
                chunk = self.metadata[idx]
                chunk_id = chunk.get("id")
                
                if chunk_id and chunk_id not in seen_ids:
                    seen_ids.add(chunk_id)
                    original_idx_in_results = indices[0].tolist().index(idx)
                    distance = float(distances[0][original_idx_in_results])
                    
                    results.append({
                        "id": chunk_id,
                        "source": chunk.get("source"),
                        "date": chunk.get("date"),
                        "content": chunk.get("content"),
                        "distance": distance
                    })
            
            # å†æ¬¡æ’åº
            results.sort(key=lambda x: x["distance"])
        
        total_time = time.time() - start_time
        logger.debug(f"ğŸ“Š æœ€ç»ˆç»“æœ: {len(results)} æ¡ï¼ˆå»é‡åï¼‰ï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
        return results[:top_k]  # ç¡®ä¿ä¸è¶…è¿‡ top_k
