#!/usr/bin/env python3
"""
Agent ä¸»ç¨‹åº - ReAct æ¨¡å¼ç‰ˆ
ä¸ä¾èµ–ä¸ç¨³å®šçš„ Native Tool Callingï¼Œé€šè¿‡æ–‡æœ¬åè®®å®ç° Agentic RAG
"""

import os
import re
import json
import logging
from logging.handlers import RotatingFileHandler
import requests
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from openai import OpenAI
from dotenv import load_dotenv
from retriever import VectorRetriever

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# ================= é…ç½®ä¸æ—¥å¿— =================
LOG_DIR = os.getenv("LOG_DIR", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

# é…ç½®æ—¥å¿—ï¼ˆä½¿ç”¨è½®è½¬æ—¥å¿—å¤„ç†å™¨ï¼Œé™åˆ¶å•ä¸ªæ–‡ä»¶å¤§å°ä¸º 10MBï¼Œä¿ç•™ 5 ä¸ªå¤‡ä»½æ–‡ä»¶ï¼‰
log_format = logging.Formatter(
    '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# ä½¿ç”¨ RotatingFileHandler å®ç°æ—¥å¿—è½®è½¬
# maxBytes: å•ä¸ªæ—¥å¿—æ–‡ä»¶æœ€å¤§ 10MB
# backupCount: ä¿ç•™ 5 ä¸ªå¤‡ä»½æ–‡ä»¶
file_handler = RotatingFileHandler(
    os.path.join(LOG_DIR, 'agent.log'),
    maxBytes=10 * 1024 * 1024,  # 10MB
    backupCount=5,
    encoding='utf-8'
)
file_handler.setLevel(logging.DEBUG)
file_handler.setFormatter(log_format)

console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(log_format)

logger = logging.getLogger("vector_indexer.agent")
logger.setLevel(logging.DEBUG)
logger.addHandler(file_handler)
logger.addHandler(console_handler)

API_BASE_URL = "https://space.ai-builders.com/backend/v1"
RETRIEVER_URL = os.getenv("RETRIEVER_URL", "http://localhost:8000")
API_KEY = os.getenv("AI_BUILDER_TOKEN")

# æœ¬åœ°æ£€ç´¢å™¨é…ç½®ï¼ˆä¼˜å…ˆä½¿ç”¨æœ¬åœ°ç›´è¿æ¨¡å¼ï¼‰
INDEX_PATH = os.getenv("INDEX_PATH", "my_history.index")
METADATA_PATH = os.getenv("METADATA_PATH", "chunks_metadata.json")

# å…¨å±€æ£€ç´¢å™¨å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
_local_retriever: Optional[VectorRetriever] = None

# ================= æ ¸å¿ƒå·¥å…·å‡½æ•° =================

def rewrite_query_with_context(query: str, conversation_history: Optional[List[Dict[str, Any]]] = None) -> str:
    """
    åŸºäºå¯¹è¯å†å²é‡å†™æŸ¥è¯¢ï¼Œå¢å¼ºä¸Šä¸‹æ–‡ç†è§£
    
    Args:
        query: åŸå§‹æŸ¥è¯¢
        conversation_history: å¯¹è¯å†å²
        
    Returns:
        ä¼˜åŒ–åçš„æŸ¥è¯¢
    """
    if not conversation_history or len(conversation_history) == 0:
        return query
    
    # æå–æœ€è¿‘å¯¹è¯ä¸­çš„å…³é”®ä¿¡æ¯
    recent_context = []
    for msg in conversation_history[-6:]:  # æœ€è¿‘3è½®å¯¹è¯
        content = msg.get("content", "")
        if content:
            # æå–æ—¥æœŸã€äººåã€äº‹ä»¶ç­‰å…³é”®ä¿¡æ¯
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„NLPå¤„ç†ï¼Œä½†ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬å…ˆæå–æ˜æ˜¾çš„æ—¥æœŸå’Œå…³é”®è¯
            recent_context.append(content[:200])  # ä¿ç•™å‰200å­—ç¬¦
    
    # å¦‚æœæŸ¥è¯¢å¾ˆçŸ­æˆ–åŒ…å«ä»£è¯ï¼Œå°è¯•ä»å†å²ä¸­è¡¥å……ä¿¡æ¯
    if len(query) < 10 or any(word in query for word in ["å®ƒ", "é‚£ä¸ª", "è¿™ä¸ª", "é‚£å¤©", "é‚£æ—¶å€™", "ä¹‹å‰"]):
        context_text = " ".join(recent_context)
        # ç®€å•çš„å¯å‘å¼ï¼šå¦‚æœå†å²ä¸­æœ‰æ—¥æœŸï¼Œå¯ä»¥è¡¥å……åˆ°æŸ¥è¯¢ä¸­
        # å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„NLPæ¨¡å‹
        logger.debug(f"ğŸ”„ æŸ¥è¯¢é‡å†™: åŸå§‹='{query}', ä¸Šä¸‹æ–‡é•¿åº¦={len(context_text)}")
    
    return query

def normalize_date(date_str: str, current_date: datetime) -> Optional[str]:
    """
    å°†ç›¸å¯¹æ—¥æœŸï¼ˆå¦‚ "yesterday", "last_month"ï¼‰è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
    æ³¨æ„ï¼šå¯¹äº N_days_ago æ ¼å¼ï¼Œç›´æ¥è¿”å›åŸæ ¼å¼ï¼Œè®© retriever._parse_date_filter å¤„ç†
    """
    if not date_str or date_str.lower() == "none":
        return None
    
    date_str = date_str.strip()
    date_str_lower = date_str.lower()
    
    # å¤„ç†ç›¸å¯¹æ—¶é—´
    if date_str_lower == "yesterday":
        yesterday = current_date - timedelta(days=1)
        return yesterday.strftime("%Y-%m-%d")
    elif date_str_lower == "today":
        return current_date.strftime("%Y-%m-%d")
    elif date_str_lower == "last_week":
        last_week = current_date - timedelta(days=7)
        return last_week.strftime("%Y-%m-%d")
    elif date_str_lower == "last_month":
        if current_date.month == 1:
            last_month = current_date.replace(year=current_date.year - 1, month=12)
        else:
            last_month = current_date.replace(month=current_date.month - 1)
        return last_month.strftime("%Y-%m")
    elif date_str_lower == "last_year":
        return str(current_date.year - 1)
    
    # å¤„ç† "N_days_ago" æ ¼å¼ï¼ˆå¦‚ "2_days_ago"ï¼‰ï¼Œç›´æ¥è¿”å›è®© retriever å¤„ç†
    if date_str_lower.endswith("_days_ago"):
        return date_str  # ä¿æŒåŸæ ¼å¼ï¼Œè®© retriever._parse_date_filter å¤„ç†
    
    # å¤„ç† "N_months_ago" æ ¼å¼
    if date_str_lower.endswith("_months_ago"):
        return date_str  # ä¿æŒåŸæ ¼å¼ï¼Œè®© retriever._parse_date_filter å¤„ç†
    
    # å¦‚æœå·²ç»æ˜¯æ ‡å‡†æ ¼å¼ï¼Œç›´æ¥è¿”å›
    # æ”¯æŒ YYYY-MM-DD, YYYY-MM, YYYY-MM-ä¸‹æ—¬ ç­‰æ ¼å¼
    return date_str

def _match_date_filter(item_date: Any, filter_date: str, current_date: datetime) -> bool:
    """
    æ£€æŸ¥ item_date æ˜¯å¦åŒ¹é… date_filter
    
    Args:
        item_date: è®°å½•ä¸­çš„æ—¥æœŸï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²ã€Noneç­‰ï¼‰
        filter_date: è¿‡æ»¤æ¡ä»¶ï¼ˆå¦‚ "2024-04-25", "2024-04", "2024-11-ä¸‹æ—¬" ç­‰ï¼‰
        current_date: å½“å‰æ—¥æœŸ
        
    Returns:
        æ˜¯å¦åŒ¹é…
    """
    if item_date is None:
        # å¦‚æœè®°å½•æ—¥æœŸä¸ºNoneï¼Œæ ¹æ®è¿‡æ»¤æ¡ä»¶å†³å®šæ˜¯å¦åŒ¹é…
        # å¦‚æœè¿‡æ»¤æ¡ä»¶å¾ˆå…·ä½“ï¼ˆå¦‚å…·ä½“æ—¥æœŸï¼‰ï¼Œåˆ™ä¸åŒ¹é…
        # å¦‚æœè¿‡æ»¤æ¡ä»¶å¾ˆå®½æ³›ï¼ˆå¦‚å¹´ä»½ï¼‰ï¼Œåˆ™å¯ä»¥åŒ¹é…
        return False  # ä¿å®ˆç­–ç•¥ï¼šNoneæ—¥æœŸä¸åŒ¹é…ä»»ä½•è¿‡æ»¤æ¡ä»¶
    
    if not isinstance(item_date, str):
        return False
    
    item_date_str = item_date.strip()
    if not item_date_str:
        return False
    
    # å¤„ç†"ä¸‹æ—¬"ç­‰ç‰¹æ®Šæ ¼å¼
    if filter_date.endswith('-ä¸‹æ—¬'):
        # æå–å¹´æœˆ
        year_month = filter_date[:-3]  # "2024-11"
        if item_date_str.startswith(year_month):
            # æ£€æŸ¥æ—¥æœŸæ˜¯å¦åœ¨21-31æ—¥ä¹‹é—´
            try:
                if len(item_date_str) >= 10:  # YYYY-MM-DDæ ¼å¼
                    day = int(item_date_str.split('-')[2])
                    return 21 <= day <= 31
            except:
                pass
        return False
    
    # ç®€å•åŒ¹é…ï¼šæ£€æŸ¥item_dateæ˜¯å¦ä»¥filter_dateå¼€å¤´
    return item_date_str.startswith(filter_date)

def _get_local_retriever() -> Optional[VectorRetriever]:
    """
    è·å–æœ¬åœ°æ£€ç´¢å™¨å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
    
    Returns:
        VectorRetriever å®ä¾‹ï¼Œå¦‚æœåˆå§‹åŒ–å¤±è´¥åˆ™è¿”å› None
    """
    global _local_retriever
    
    # å¦‚æœå·²ç»åˆå§‹åŒ–ï¼Œç›´æ¥è¿”å›
    if _local_retriever is not None:
        return _local_retriever
    
    # æ£€æŸ¥å¿…è¦çš„é…ç½®
    if not API_KEY:
        logger.warning("âš ï¸  API_KEY æœªè®¾ç½®ï¼Œæ— æ³•åˆå§‹åŒ–æœ¬åœ°æ£€ç´¢å™¨")
        return None
    
    if not os.path.exists(INDEX_PATH):
        logger.warning(f"âš ï¸  ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {INDEX_PATH}ï¼Œæ— æ³•ä½¿ç”¨æœ¬åœ°æ£€ç´¢å™¨")
        return None
    
    if not os.path.exists(METADATA_PATH):
        logger.warning(f"âš ï¸  å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {METADATA_PATH}ï¼Œæ— æ³•ä½¿ç”¨æœ¬åœ°æ£€ç´¢å™¨")
        return None
    
    # åˆå§‹åŒ–æœ¬åœ°æ£€ç´¢å™¨
    try:
        logger.info(f"ğŸ”§ [æœ¬åœ°æ¨¡å¼] æ­£åœ¨åˆå§‹åŒ–æœ¬åœ°æ£€ç´¢å™¨...")
        logger.info(f"   ç´¢å¼•æ–‡ä»¶: {INDEX_PATH}")
        logger.info(f"   å…ƒæ•°æ®æ–‡ä»¶: {METADATA_PATH}")
        _local_retriever = VectorRetriever(INDEX_PATH, METADATA_PATH, API_KEY)
        logger.info(f"âœ… [æœ¬åœ°æ¨¡å¼] æœ¬åœ°æ£€ç´¢å™¨åˆå§‹åŒ–æˆåŠŸï¼ç´¢å¼•å‘é‡æ•°: {_local_retriever.index.ntotal}")
        return _local_retriever
    except Exception as e:
        logger.exception(f"âŒ [æœ¬åœ°æ¨¡å¼] æœ¬åœ°æ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return None

def call_retriever(query: str, date_filter: Optional[str] = None, max_results: int = 10, expand_query: bool = False) -> str:
    """
    è°ƒç”¨æ£€ç´¢æœåŠ¡ï¼Œå®æ–½æ··åˆæ£€ç´¢ç­–ç•¥ï¼ˆå…³é”®è¯æ£€ç´¢ + å‘é‡æ£€ç´¢ï¼‰
    
    ä¿®å¤å®ä½“æ··æ·†å¹»è§‰é—®é¢˜ï¼šé€šè¿‡å…³é”®è¯æš´åŠ›æ£€ç´¢ç¡®ä¿ç¨€æœ‰äººåï¼ˆå¦‚"å¼ ä¸‰"ï¼‰èƒ½è¢«å‡†ç¡®å¬å›ã€‚
    
    Args:
        query: æŸ¥è¯¢æ–‡æœ¬
        date_filter: æ—¥æœŸè¿‡æ»¤æ¡ä»¶ï¼ˆå¯ä»¥æ˜¯ç›¸å¯¹æ—¶é—´å¦‚ "yesterday" æˆ–æ ‡å‡†æ ¼å¼ï¼‰
        max_results: æœ€å¤§ç»“æœæ•°
        expand_query: æ˜¯å¦æ‰©å±•æŸ¥è¯¢ï¼ˆå¦‚æœç¬¬ä¸€æ¬¡æ£€ç´¢ç»“æœä¸ç†æƒ³ï¼Œå¯ä»¥å°è¯•æ‰©å±•æŸ¥è¯¢ï¼‰
        
    Returns:
        æ ¼å¼åŒ–åçš„æ£€ç´¢ç»“æœå­—ç¬¦ä¸²
    """
    # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
    current_date = datetime.now()
    normalized_date = normalize_date(date_filter, current_date) if date_filter else None
    
    # å¦‚æœå¯ç”¨æŸ¥è¯¢æ‰©å±•ï¼Œå°è¯•æ·»åŠ ç›¸å…³å…³é”®è¯
    search_query = query
    if expand_query:
        # ç®€å•çš„æŸ¥è¯¢æ‰©å±•ï¼šæ·»åŠ åŒä¹‰è¯æˆ–ç›¸å…³è¯
        # å®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„NLPæ¨¡å‹
        query_words = query.split()
        # å¯ä»¥æ·»åŠ åŒä¹‰è¯è¯å…¸æˆ–ä½¿ç”¨embeddingç›¸ä¼¼åº¦æ‰©å±•
        logger.debug(f"ğŸ” [æŸ¥è¯¢æ‰©å±•] åŸå§‹æŸ¥è¯¢: '{query}'")
    
    logger.info(f"ğŸ” [å·¥å…·æ‰§è¡Œ] æ­£åœ¨æ£€ç´¢: Query='{search_query}', Date='{normalized_date}', Expand={expand_query}")
    
    # ========== ä¼˜å…ˆä½¿ç”¨æœ¬åœ°ç›´è¿æ¨¡å¼ ==========
    local_retriever = _get_local_retriever()
    if local_retriever is not None:
        try:
            logger.debug("ğŸš€ [æœ¬åœ°æ¨¡å¼] ä½¿ç”¨æ··åˆæ£€ç´¢ç­–ç•¥ï¼ˆå…³é”®è¯ + å‘é‡ï¼‰")
            
            keyword_results = []
            vector_results = []
            
            # ========== 1. å…³é”®è¯æš´åŠ›æ£€ç´¢ï¼ˆé’ˆå¯¹ç¨€æœ‰äººå/ä¸“æœ‰åè¯ï¼‰==========
            if len(query.strip()) < 20:  # ç–‘ä¼¼äººåæˆ–ä¸“æœ‰åè¯
                logger.debug(f"ğŸ”‘ [å…³é”®è¯æ£€ç´¢] æŸ¥è¯¢é•¿åº¦ {len(query.strip())} < 20ï¼Œå¯ç”¨å…³é”®è¯æš´åŠ›æ£€ç´¢")
                query_stripped = query.strip()
                query_lower = query_stripped.lower()
                
                # å°†æŸ¥è¯¢è¯æ‹†åˆ†ä¸ºå…³é”®è¯åˆ—è¡¨ï¼ˆæŒ‰ç©ºæ ¼åˆ†å‰²ï¼‰
                query_keywords = [kw.strip() for kw in query_stripped.split() if kw.strip()]
                is_multi_word = len(query_keywords) > 1
                
                # éå†æ‰€æœ‰å…ƒæ•°æ®ï¼Œè¿›è¡Œå…³é”®è¯åŒ¹é…
                if hasattr(local_retriever, 'metadata') and local_retriever.metadata:
                    for item in local_retriever.metadata:
                        content = item.get('content', '')
                        if not content:
                            continue
                        
                        content_lower = content.lower()
                        
                        # åŒ¹é…é€»è¾‘ï¼š
                        # - å•è¯æŸ¥è¯¢ï¼šè¦æ±‚å®Œæ•´åŒ¹é…ï¼ˆå¦‚ "å¼ ä¸‰"ï¼‰
                        # - å¤šè¯æŸ¥è¯¢ï¼šè¦æ±‚åŒ…å«æ‰€æœ‰å…³é”®è¯ï¼ˆå¦‚ "å†…å¿ƒçš„å°å­© åå­—" éœ€è¦åŒ…å« "å†…å¿ƒçš„å°å­©" å’Œ "åå­—"ï¼‰
                        if is_multi_word:
                            # å¤šè¯æŸ¥è¯¢ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«æ‰€æœ‰å…³é”®è¯
                            if all(kw.lower() in content_lower for kw in query_keywords):
                                match_found = True
                            else:
                                match_found = False
                        else:
                            # å•è¯æŸ¥è¯¢ï¼šå®Œæ•´åŒ¹é…
                            match_found = query_lower in content_lower
                        
                        if match_found:
                            # åº”ç”¨æ—¥æœŸè¿‡æ»¤ï¼ˆå¦‚æœæœ‰ï¼‰
                            item_date = item.get('date')
                            if normalized_date:
                                if not _match_date_filter(item_date, normalized_date, current_date):
                                    continue
                            
                            # åˆ›å»ºå…³é”®è¯åŒ¹é…ç»“æœ
                            keyword_result = item.copy()
                            keyword_result['_source'] = 'keyword_match'  # æ ‡è®°ä¸ºå…³é”®è¯åŒ¹é…
                            keyword_result['distance'] = 0.0  # å…³é”®è¯åŒ¹é…è·ç¦»ä¸º0ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
                            keyword_results.append(keyword_result)
                    
                    logger.debug(f"ğŸ”‘ [å…³é”®è¯æ£€ç´¢] æ‰¾åˆ° {len(keyword_results)} æ¡å…³é”®è¯åŒ¹é…ç»“æœ")
            
            # ========== 2. å‘é‡æ£€ç´¢ï¼ˆåŸæœ‰é€»è¾‘ + Post-Retrieval Filteringï¼‰==========
            try:
                vector_results_raw = local_retriever.search(
                    query=search_query,
                    top_k=max_results,
                    date_filter=normalized_date,
                    current_date=current_date
                )
                
                # ========== Post-Retrieval Filtering: æ£€ç´¢åæ¸…æ´— ==========
                # å¦‚æœæŸ¥è¯¢è¯å¾ˆçŸ­ï¼Œè§†ä¸ºç²¾å‡†å®ä½“æŸ¥è¯¢ï¼Œå¼ºåˆ¶æ£€æŸ¥å†…å®¹æ˜¯å¦åŒ…å«æŸ¥è¯¢è¯
                query_stripped = query.strip()
                is_precise_entity_query = len(query_stripped) < 15
                
                vector_results_filtered = []
                
                # å°†æŸ¥è¯¢è¯æ‹†åˆ†ä¸º token åˆ—è¡¨ï¼ˆæŒ‰ç©ºæ ¼åˆ†å‰²ï¼‰
                query_tokens = [token.strip() for token in query_stripped.split() if token.strip()]
                
                # æ‰¾åˆ°æœ€é•¿çš„ tokenï¼ˆæ ¸å¿ƒå®ä½“ï¼‰
                longest_token = max(query_tokens, key=len) if query_tokens else query_stripped
                longest_token_lower = longest_token.lower()
                
                for r in vector_results_raw:
                    # æ ‡è®°å‘é‡æ£€ç´¢ç»“æœ
                    r['_source'] = 'vector_search'
                    
                    # å¦‚æœæ˜¯ç²¾å‡†å®ä½“æŸ¥è¯¢ï¼Œè¿›è¡Œæ¸…æ´—
                    if is_precise_entity_query:
                        content = r.get('content', '')
                        if not content:
                            logger.warning(f"âš ï¸  [æ£€ç´¢æ¸…æ´—] ä¸¢å¼ƒç»“æœ ID={r.get('id')}ï¼šå†…å®¹ä¸ºç©º")
                            continue
                        
                        content_lower = content.lower()
                        # è·å– source å­—æ®µï¼Œç¡®ä¿æ˜¯å­—ç¬¦ä¸²ç±»å‹
                        source_raw = r.get('source')
                        source = str(source_raw).strip() if source_raw is not None else ''
                        
                        # è·å–è®°å½• ID
                        record_id = r.get('id', '')
                        
                        # äº§å“å±‚é¢ï¼švoice è®°å½•æ˜¯æ ¸å¿ƒæ•°æ®æºï¼Œå¿…é¡»èƒ½è¢«æ£€ç´¢åˆ°
                        # ä½¿ç”¨æ›´å¯é çš„åˆ¤æ–­æ–¹å¼ï¼šåŒæ—¶æ£€æŸ¥ ID å‰ç¼€å’Œ source å­—æ®µ
                        # ä¼˜å…ˆæ£€æŸ¥ ID å‰ç¼€ï¼ˆæ›´å¯é ï¼‰ï¼Œç„¶åæ£€æŸ¥ source å­—æ®µ
                        is_voice = (
                            record_id.startswith('voice_') or 
                            record_id == 'test_manual_001' or
                            source == 'voice'
                        )
                        
                        # è°ƒè¯•ï¼šæ£€æŸ¥ voice è®°å½•
                        if is_voice:
                            logger.info(f"ğŸ” [è°ƒè¯•] æ£€æµ‹åˆ° voice è®°å½•: ID={record_id}, source={repr(source)}, is_voice={is_voice}")
                        
                        # å¯¹äº voice æ¥æºçš„è®°å½•ï¼Œæ”¾å®½æ¸…æ´—æ¡ä»¶
                        # å› ä¸º voice è®°å½•çš„å†…å®¹æ˜¯ç”¨æˆ·ç›´æ¥è¯´çš„ï¼Œå¯èƒ½ä¸åŒ…å«æŸ¥è¯¢ä¸­çš„æŸäº›ä¿®é¥°è¯
                        # ç‰¹åˆ«åœ°ï¼Œå¯¹äºå®½æ³›æŸ¥è¯¢ï¼ˆå¦‚"è®°å½•"ã€"å†…å®¹"ã€"æœ€è¿‘"ï¼‰ï¼Œvoice è®°å½•åº”è¯¥å…¨éƒ¨ä¿ç•™
                        if is_voice:
                            # å®šä¹‰é€šç”¨æŸ¥è¯¢è¯åˆ—è¡¨ï¼ˆè¿™äº›è¯å‡ºç°æ—¶ï¼Œvoice è®°å½•ä¸è¿‡æ»¤ï¼‰
                            generic_words = {'è®°å½•', 'å†…å®¹', 'voice', 'å½•éŸ³', 'è¯­éŸ³', 'å¤‡å¿˜', 'æœ€è¿‘', 'ä»€ä¹ˆ', 'å“ªäº›', 'æœ‰ä»€ä¹ˆ'}
                            
                            # æ£€æŸ¥æŸ¥è¯¢æ˜¯å¦åŒ…å«é€šç”¨è¯ï¼Œæˆ–è€…æŸ¥è¯¢å¾ˆçŸ­ï¼ˆå¯èƒ½æ˜¯å®½æ³›æŸ¥è¯¢ï¼‰
                            is_generic_query = (
                                any(token.lower() in generic_words for token in query_tokens) or 
                                len(query_stripped) <= 6 or
                                'æœ€è¿‘' in query_stripped or
                                'ä»€ä¹ˆ' in query_stripped
                            )
                            
                            if is_generic_query:
                                # å¯¹äºé€šç”¨/å®½æ³›æŸ¥è¯¢ï¼Œvoice è®°å½•å…¨éƒ¨ä¿ç•™ï¼ˆä¸è¿‡æ»¤ï¼‰
                                logger.debug(f"âœ… [æ£€ç´¢æ¸…æ´—] ä¿ç•™ voice è®°å½• ID={r.get('id')}ï¼ˆé€šç”¨æŸ¥è¯¢ä¸è¿‡æ»¤ï¼‰")
                                pass  # ä¸è¿›è¡Œè¿‡æ»¤ï¼Œç›´æ¥ä¿ç•™
                            else:
                                # å¯¹äºå…·ä½“æŸ¥è¯¢ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«ä»»æ„å…³é”®è¯ï¼ˆè‡³å°‘ä¸€ä¸ªè¯é•¿åº¦>1ï¼‰
                                has_any_keyword = any(
                                    token.lower() in content_lower 
                                    for token in query_tokens 
                                    if len(token) > 1
                                )
                                if not has_any_keyword:
                                    logger.warning(f"âš ï¸  [æ£€ç´¢æ¸…æ´—] ä¸¢å¼ƒ voice è®°å½• ID={r.get('id')}, æ—¥æœŸ={r.get('date')}ï¼šå†…å®¹ä¸å«ä»»ä½•æŸ¥è¯¢å…³é”®è¯ (æŸ¥è¯¢: '{query_stripped}')")
                                    continue
                        else:
                            # å¯¹äºå…¶ä»–æ¥æºçš„è®°å½•ï¼Œä½¿ç”¨ä¸¥æ ¼çš„å®ä½“åŒ¹é…ï¼šæ£€æŸ¥æœ€é•¿ tokenï¼ˆæ ¸å¿ƒå®ä½“ï¼‰æ˜¯å¦åœ¨å†…å®¹ä¸­
                            # ç¤ºä¾‹ï¼š
                            # - Query: "å¼ ä¸‰" -> Longest: "å¼ ä¸‰" -> æ–‡æ¡£æ—  å¼ ä¸‰ -> ä¸¢å¼ƒ (æ­£ç¡®)
                            # - Query: "å†…å¿ƒçš„å°å­© åå­—" -> Longest: "å†…å¿ƒçš„å°å­©" -> æ–‡æ¡£æœ‰ "å†…å¿ƒçš„å°å­©" -> ä¿ç•™ (ä¿®å¤ç›®æ ‡)
                            if longest_token_lower not in content_lower:
                                logger.warning(f"âš ï¸  [æ£€ç´¢æ¸…æ´—] ä¸¢å¼ƒç»“æœ ID={r.get('id')}, æ—¥æœŸ={r.get('date')}ï¼šå†…å®¹ä¸å«æ ¸å¿ƒå®ä½“ '{longest_token}' (æŸ¥è¯¢: '{query_stripped}')")
                                continue
                    
                    # é€šè¿‡æ¸…æ´—ï¼Œæ·»åŠ åˆ°ç»“æœåˆ—è¡¨
                    vector_results_filtered.append(r)
                
                vector_results = vector_results_filtered
                
                if is_precise_entity_query:
                    filtered_count = len(vector_results_raw) - len(vector_results)
                    logger.debug(f"ğŸ” [å‘é‡æ£€ç´¢] åŸå§‹ç»“æœ: {len(vector_results_raw)} æ¡ï¼Œæ¸…æ´—å: {len(vector_results)} æ¡ï¼Œä¸¢å¼ƒ: {filtered_count} æ¡")
                else:
                    logger.debug(f"ğŸ” [å‘é‡æ£€ç´¢] æ‰¾åˆ° {len(vector_results)} æ¡å‘é‡æ£€ç´¢ç»“æœ")
                    
            except Exception as e:
                logger.warning(f"âš ï¸  [å‘é‡æ£€ç´¢] å‘é‡æ£€ç´¢å¤±è´¥: {e}")
                vector_results = []
            
            # ========== 3. åˆå¹¶ä¸å»é‡ ==========
            # å…³é”®è¯ç»“æœä¼˜å…ˆï¼Œç„¶åå‘é‡ç»“æœ
            all_results = []
            seen_ids = set()
            
            # å…ˆæ·»åŠ å…³é”®è¯åŒ¹é…ç»“æœï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
            for r in keyword_results:
                result_id = r.get('id')
                if result_id and result_id not in seen_ids:
                    seen_ids.add(result_id)
                    all_results.append(r)
            
            # å†æ·»åŠ å‘é‡æ£€ç´¢ç»“æœï¼ˆå»é‡ï¼‰
            for r in vector_results:
                result_id = r.get('id')
                if result_id and result_id not in seen_ids:
                    seen_ids.add(result_id)
                    all_results.append(r)
            
            # é™åˆ¶æ€»æ•°ä¸è¶…è¿‡ max_resultsï¼ˆé»˜è®¤10ï¼Œä½†ä¸ºäº†æ··åˆæ£€ç´¢ï¼Œæˆ‘ä»¬å…è®¸ç¨å¤šä¸€äº›ï¼‰
            final_results = all_results[:max_results]
            
            # ========== 4. Query Relaxation: æŸ¥è¯¢æ”¾æ¾ç­–ç•¥ ==========
            # å¦‚æœç»“æœä¸ºç©ºï¼Œä¸”ä½¿ç”¨äº†æ—¥æœŸè¿‡æ»¤ï¼Œå°è¯•ç§»é™¤æ—¥æœŸé™åˆ¶é‡æ–°æ£€ç´¢
            if not final_results and normalized_date and date_filter:
                logger.info(f"ğŸ”„ [Query Relaxation] å¸¦æ—¥æœŸæ£€ç´¢å¤±è´¥ï¼Œå°è¯•ç§»é™¤æ—¥æœŸé™åˆ¶é‡æ–°æ£€ç´¢: query='{query}', åŸæ—¥æœŸè¿‡æ»¤='{date_filter}'")
                # é€’å½’è°ƒç”¨è‡ªå·±ï¼Œä½†æŠŠ date_filter è®¾ä¸º None
                # æ³¨æ„ï¼šè¿™é‡Œä¼ å…¥çš„æ˜¯åŸå§‹ queryï¼Œè€Œä¸æ˜¯ search_queryï¼Œå› ä¸º search_query å¯èƒ½è¢« expand_query ä¿®æ”¹è¿‡
                relaxed_result = call_retriever(query, date_filter=None, max_results=max_results, expand_query=expand_query)
                # å¦‚æœæ”¾æ¾æŸ¥è¯¢æ‰¾åˆ°äº†ç»“æœï¼Œç›´æ¥è¿”å›
                if relaxed_result and "æ²¡æœ‰æ‰¾åˆ°" not in relaxed_result and "å®Œå…¨æ²¡æœ‰" not in relaxed_result:
                    logger.info(f"âœ… [Query Relaxation] ç§»é™¤æ—¥æœŸé™åˆ¶åæ‰¾åˆ°ç»“æœ")
                    return relaxed_result
                # å¦‚æœæ”¾æ¾æŸ¥è¯¢ä»ç„¶æ²¡æœ‰ç»“æœï¼Œç»§ç»­æ‰§è¡Œé˜²å¹»è§‰å…œåº•
            
            # ========== 5. é˜²å¹»è§‰å…œåº• ==========
            if not final_results:
                logger.warning(f"âš ï¸  [é˜²å¹»è§‰] æ£€ç´¢ç»“æœä¸ºç©ºï¼Œè¿”å›é˜²å¹»è§‰å…œåº•æ¶ˆæ¯")
                return "ã€ç³»ç»Ÿåé¦ˆã€‘æ•°æ®åº“ä¸­**å®Œå…¨æ²¡æœ‰**æ‰¾åˆ°åŒ…å«æ­¤å…³é”®è¯çš„è®°å½•ã€‚è¯·ç›´æ¥å‘Šè¯‰ç”¨æˆ·'æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®°å½•'ï¼Œ**ä¸¥ç¦**æåŠå…¶ä»–æ— å…³äººç‰©ï¼Œ**ä¸¥ç¦**ç¼–é€ å…³ç³»ã€‚"
            
            # ========== 6. æ ¼å¼åŒ–ç»“æœä¾› LLM é˜…è¯» ==========
            context_lines = ["ã€ç³»ç»Ÿåé¦ˆã€‘å·²æ‰¾åˆ°ä»¥ä¸‹ç›¸å…³è®°å½•ï¼š\n"]
            for i, r in enumerate(final_results, 1):
                date_str = r.get('date', 'æœªçŸ¥æ—¥æœŸ')
                content = r.get('content', '')
                distance = r.get('distance', 0.0)
                source_type = r.get('_source', 'unknown')
                
                # æˆªæ–­è¿‡é•¿çš„å†…å®¹
                if len(content) > 500:
                    content = content[:500] + "..."
                
                # æ˜¾ç¤ºæ£€ç´¢æ¥æºï¼ˆå…³é”®è¯åŒ¹é…æˆ–å‘é‡æ£€ç´¢ï¼‰
                source_label = "å…³é”®è¯åŒ¹é…" if source_type == 'keyword_match' else "å‘é‡æ£€ç´¢"
                context_lines.append(f"--- è®°å½• {i} [æ—¥æœŸ: {date_str}, ç›¸ä¼¼åº¦: {distance:.4f}, æ¥æº: {source_label}] ---\n{content}\n")
            
            logger.info(f"âœ… [æ··åˆæ£€ç´¢] æ£€ç´¢æˆåŠŸï¼Œå…³é”®è¯åŒ¹é…: {len(keyword_results)}, å‘é‡æ£€ç´¢: {len(vector_results)}, æœ€ç»ˆç»“æœ: {len(final_results)}")
            return "\n".join(context_lines)
            
        except Exception as e:
            logger.exception(f"âŒ [æœ¬åœ°æ¨¡å¼] æœ¬åœ°æ£€ç´¢å¤±è´¥: {e}")
            logger.info("ğŸ”„ [å…œåº•æ¨¡å¼] åˆ‡æ¢åˆ° HTTP è°ƒç”¨æ¨¡å¼")
            # ç»§ç»­æ‰§è¡Œ HTTP è°ƒç”¨ä½œä¸ºå…œåº•
    
    # ========== å…œåº•ï¼šä½¿ç”¨ HTTP è°ƒç”¨ ==========
    url = f"{RETRIEVER_URL}/retrieve"
    payload = {
        "query": search_query,
        "max_results": max_results
    }
    
    if normalized_date:
        payload["date_filter"] = normalized_date
    
    try:
        logger.debug("ğŸŒ [HTTPæ¨¡å¼] ä½¿ç”¨ HTTP è°ƒç”¨æ£€ç´¢æœåŠ¡")
        # å¢åŠ è¶…æ—¶æ—¶é—´ï¼šæ£€ç´¢å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼ˆç‰¹åˆ«æ˜¯æ—¥æœŸè¿‡æ»¤æ—¶ï¼‰
        response = requests.post(url, json=payload, timeout=120)  # å¢åŠ åˆ°120ç§’
        response.raise_for_status()
        data = response.json()
        results = data.get("results", [])
        
        # Query Relaxation: å¦‚æœç»“æœä¸ºç©ºä¸”ä½¿ç”¨äº†æ—¥æœŸè¿‡æ»¤ï¼Œå°è¯•ç§»é™¤æ—¥æœŸé™åˆ¶é‡æ–°æ£€ç´¢
        if not results and normalized_date and date_filter:
            logger.info(f"ğŸ”„ [HTTPæ¨¡å¼ Query Relaxation] å¸¦æ—¥æœŸæ£€ç´¢å¤±è´¥ï¼Œå°è¯•ç§»é™¤æ—¥æœŸé™åˆ¶é‡æ–°æ£€ç´¢: query='{query}', åŸæ—¥æœŸè¿‡æ»¤='{date_filter}'")
            # é€’å½’è°ƒç”¨è‡ªå·±ï¼Œä½†æŠŠ date_filter è®¾ä¸º None
            relaxed_result = call_retriever(query, date_filter=None, max_results=max_results, expand_query=expand_query)
            # å¦‚æœæ”¾æ¾æŸ¥è¯¢æ‰¾åˆ°äº†ç»“æœï¼Œç›´æ¥è¿”å›
            if relaxed_result and "æ²¡æœ‰æ‰¾åˆ°" not in relaxed_result and "å®Œå…¨æ²¡æœ‰" not in relaxed_result:
                logger.info(f"âœ… [HTTPæ¨¡å¼ Query Relaxation] ç§»é™¤æ—¥æœŸé™åˆ¶åæ‰¾åˆ°ç»“æœ")
                return relaxed_result
            # å¦‚æœæ”¾æ¾æŸ¥è¯¢ä»ç„¶æ²¡æœ‰ç»“æœï¼Œç»§ç»­æ‰§è¡Œé˜²å¹»è§‰å…œåº•
        
        # é˜²å¹»è§‰å…œåº•
        if not results:
            return "ã€ç³»ç»Ÿåé¦ˆã€‘æ•°æ®åº“ä¸­**å®Œå…¨æ²¡æœ‰**æ‰¾åˆ°åŒ…å«æ­¤å…³é”®è¯çš„è®°å½•ã€‚è¯·ç›´æ¥å‘Šè¯‰ç”¨æˆ·'æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®°å½•'ï¼Œ**ä¸¥ç¦**æåŠå…¶ä»–æ— å…³äººç‰©ï¼Œ**ä¸¥ç¦**ç¼–é€ å…³ç³»ã€‚"
        
        # æ ¼å¼åŒ–ç»“æœä¾› LLM é˜…è¯»
        context_lines = ["ã€ç³»ç»Ÿåé¦ˆã€‘å·²æ‰¾åˆ°ä»¥ä¸‹ç›¸å…³è®°å½•ï¼š\n"]
        for i, r in enumerate(results, 1):
            date_str = r.get('date', 'æœªçŸ¥æ—¥æœŸ')
            content = r.get('content', '')
            distance = r.get('distance', 0.0)
            # æˆªæ–­è¿‡é•¿çš„å†…å®¹
            if len(content) > 500:
                content = content[:500] + "..."
            context_lines.append(f"--- è®°å½• {i} [æ—¥æœŸ: {date_str}, ç›¸ä¼¼åº¦: {distance:.4f}] ---\n{content}\n")
        
        logger.debug(f"âœ… [HTTPæ¨¡å¼] æ£€ç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results)} æ¡ç»“æœ")
        return "\n".join(context_lines)
        
    except requests.exceptions.RequestException as e:
        logger.exception(f"âŒ [HTTPæ¨¡å¼] æ£€ç´¢æœåŠ¡è°ƒç”¨å¤±è´¥: {e}")
        return f"ã€ç³»ç»Ÿé”™è¯¯ã€‘æ£€ç´¢æœåŠ¡æš‚æ—¶ä¸å¯ç”¨: {str(e)}"
    except Exception as e:
        logger.exception(f"âŒ [HTTPæ¨¡å¼] æ£€ç´¢å¼‚å¸¸: {e}")
        return f"ã€ç³»ç»Ÿé”™è¯¯ã€‘æ£€ç´¢è¿‡ç¨‹å‡ºç°å¼‚å¸¸: {str(e)}"

def get_system_prompt(conversation_history: Optional[List[Dict[str, Any]]] = None) -> str:
    """
    ç”Ÿæˆ System Promptï¼Œæ•™ AI ä½¿ç”¨ ReAct åè®®
    
    Args:
        conversation_history: å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰ï¼Œç”¨äºä¸Šä¸‹æ–‡ç†è§£
    """
    current_date = datetime.now()
    current_date_str = current_date.strftime("%Y-%m-%d")
    current_year = current_date.year
    current_month = current_date.month
    
    # æ„å»ºå¯¹è¯å†å²æ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰
    history_context = ""
    if conversation_history and len(conversation_history) > 0:
        # æå–æœ€è¿‘å‡ è½®å¯¹è¯çš„å…³é”®ä¿¡æ¯
        recent_messages = conversation_history[-6:]  # æœ€è¿‘3è½®å¯¹è¯
        history_summary = []
        for msg in recent_messages:
            role = msg.get("role", "")
            content = msg.get("content", "")
            if role == "user":
                # æå–ç”¨æˆ·é—®é¢˜ä¸­çš„å…³é”®ä¿¡æ¯
                history_summary.append(f"ç”¨æˆ·é—®è¿‡: {content[:100]}")
            elif role == "assistant":
                # æå–AIå›ç­”ä¸­çš„å…³é”®ä¿¡æ¯
                history_summary.append(f"æˆ‘å›ç­”è¿‡: {content[:100]}")
        
        if history_summary:
            history_context = f"""
# å¯¹è¯å†å²ä¸Šä¸‹æ–‡
ä»¥ä¸‹æ˜¯æœ€è¿‘çš„å¯¹è¯å†å²ï¼Œå¸®åŠ©ä½ ç†è§£ç”¨æˆ·çš„æ„å›¾å’Œä¸Šä¸‹æ–‡ï¼š
{chr(10).join(history_summary)}

**é‡è¦**ï¼šå½“ç”¨æˆ·ä½¿ç”¨ä»£è¯ï¼ˆå¦‚"å®ƒ"ã€"é‚£ä¸ª"ã€"è¿™ä¸ª"ï¼‰æˆ–çœç•¥ä¸»è¯­æ—¶ï¼Œè¦ç»“åˆå¯¹è¯å†å²ç†è§£ç”¨æˆ·æŒ‡çš„æ˜¯ä»€ä¹ˆã€‚
ä¾‹å¦‚ï¼Œå¦‚æœç”¨æˆ·ä¹‹å‰é—®è¿‡"2024å¹´6æœˆ2æ—¥æˆ‘åœ¨åšä»€ä¹ˆï¼Ÿ"ï¼Œç„¶åé—®"é‚£å¤©çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"ï¼Œè¿™é‡Œçš„"é‚£å¤©"æŒ‡çš„æ˜¯2024å¹´6æœˆ2æ—¥ã€‚
"""
    
    return f"""# èº«ä»½å®šä¹‰
ä½ æ˜¯æˆ‘çš„ Digital Twin å®ˆæŠ¤è€…ï¼Œä¸æ˜¯é€šç”¨çš„ ChatGPTã€‚ä½ æ‹¥æœ‰è®¿é—®æˆ‘ä¸ªäººè®°å¿†åº“çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬æˆ‘çš„æ—¥è®°ã€ç¬”è®°ã€æƒ³æ³•å’Œç»å†ã€‚ä½ çš„ä½¿å‘½æ˜¯å¸®åŠ©æˆ‘ç†è§£è‡ªå·±ã€å›å¿†è¿‡å»ã€æ´å¯Ÿæ¨¡å¼ã€‚

# å½“å‰æ—¥æœŸ
ä»Šå¤©æ˜¯ {current_date_str}ï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰

è¿™æ˜¯æœ€é‡è¦çš„ï¼ä½ å¿…é¡»å§‹ç»ˆçŸ¥é“"ä»Šå¤©"æ˜¯å“ªä¸€å¤©ï¼Œæ‰èƒ½æ­£ç¡®ç†è§£æ—¶é—´ç›¸å…³çš„æŸ¥è¯¢ï¼š
- "æ˜¨å¤©" = {(current_date - timedelta(days=1)).strftime("%Y-%m-%d")}
- "å»å¹´" = {current_year - 1}å¹´
- "ä¸Šä¸ªæœˆ" = {current_month - 1 if current_month > 1 else 12}æœˆ
{history_context}
# ä½ çš„æ€è€ƒåè®® (ReAct Protocol)

å½“ç”¨æˆ·æé—®æ—¶ï¼Œä½ å¿…é¡»å…ˆè¿›è¡Œ**æ€è€ƒ (Thought)**ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦æŸ¥è¯¢è®°å¿†åº“ã€‚

## ä»€ä¹ˆæ—¶å€™éœ€è¦æŸ¥è¯¢è®°å¿†åº“ï¼Ÿ

**å¿…é¡»æŸ¥è¯¢çš„åœºæ™¯ï¼š**
1. ç”¨æˆ·è¯¢é—®å…·ä½“æ—¥æœŸå‘ç”Ÿçš„äº‹æƒ…ï¼ˆå¦‚"2024å¹´6æœˆ2æ—¥æˆ‘åœ¨åšä»€ä¹ˆ"ï¼‰
2. ç”¨æˆ·è¯¢é—®å…³äºè¿‡å»çš„äº‹ä»¶ã€ç»å†ã€æƒ³æ³•ã€æ„Ÿå—
3. ç”¨æˆ·ä½¿ç”¨æ—¶é—´ç›¸å…³çš„è¯æ±‡ï¼ˆå¦‚"å»å¹´"ã€"ä¸Šä¸ªæœˆ"ã€"ä¹‹å‰"ã€"æ›¾ç»"ã€"é‚£å¤©"ï¼‰
4. ç”¨æˆ·è¯¢é—®å…³äºè‡ªå·±çš„æ¨¡å¼ã€ä¹ æƒ¯ã€å†³ç­–
5. ç”¨æˆ·è¯¢é—®"æˆ‘è®°å¾—..."ã€"æˆ‘å†™è¿‡..."ã€"æˆ‘ä¹‹å‰..."
6. ç”¨æˆ·çš„é—®é¢˜æ¶‰åŠä¸ªäººå†å²ã€æˆé•¿ã€å˜åŒ–
7. ç”¨æˆ·è¯¢é—®"å«ä»€ä¹ˆ"ã€"åå­—"ã€"å‘½å"ç­‰å…³äºåç§°çš„é—®é¢˜
8. ç”¨æˆ·è¯¢é—®å…³äºç‰¹å®šæ¦‚å¿µã€äººç‰©ã€äº‹ç‰©çš„åç§°æˆ–å®šä¹‰

**ä¸éœ€è¦æŸ¥è¯¢çš„åœºæ™¯ï¼š**
- çº¯ç²¹çš„çŸ¥è¯†æ€§é—®é¢˜ï¼ˆå¦‚"ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "ï¼‰
- å½“å‰æ—¶é—´çš„é—®é¢˜ï¼ˆå¦‚"ç°åœ¨å‡ ç‚¹äº†"ï¼‰
- ä¸éœ€è¦ä¸ªäººè®°å¿†çš„é€šç”¨é—®é¢˜
- ç®€å•çš„æ‰“æ‹›å‘¼ï¼ˆå¦‚"ä½ å¥½"ï¼‰

## å¦‚ä½•å‘èµ·æŸ¥è¯¢ï¼Ÿ

å¦‚æœéœ€è¦æŸ¥è¯¢è®°å¿†åº“ï¼Œè¯·**åªè¾“å‡º**ä¸€è¡Œç‰¹æ®Šçš„æŒ‡ä»¤ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

```
ACTION: SEARCH query="æŸ¥è¯¢å†…å®¹" date="æ—¥æœŸè¿‡æ»¤"
```

**å¤šè½®æ£€ç´¢ç­–ç•¥**ï¼š
- å¦‚æœç¬¬ä¸€æ¬¡æ£€ç´¢ç»“æœä¸ç†æƒ³ï¼ˆæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æˆ–ç»“æœå¤ªå°‘ï¼‰ï¼Œå¯ä»¥åœ¨å›ç­”ä¸­è¯´æ˜"è®©æˆ‘å°è¯•ç”¨ä¸åŒçš„å…³é”®è¯å†æœç´¢ä¸€æ¬¡"
- ç„¶åå†æ¬¡è¾“å‡º ACTION æŒ‡ä»¤ï¼Œä½¿ç”¨ä¸åŒçš„æŸ¥è¯¢è¯æˆ–æ›´å®½æ³›çš„æ—¥æœŸèŒƒå›´
- è¿™æ ·å¯ä»¥æé«˜æ£€ç´¢æˆåŠŸç‡

**å‚æ•°è¯´æ˜ï¼š**
- `query`: æœç´¢æŸ¥è¯¢æ–‡æœ¬ï¼Œè¦å…·ä½“æ˜ç¡®ï¼ŒåŒ…å«å…³é”®è¯
  - å¯¹äºæƒ…ç»ª/çŠ¶æ€ç±»é—®é¢˜ï¼šåŒ…å«"æŠ‘éƒ"ã€"æƒ…ç»ª"ã€"ç—‡çŠ¶"ç­‰å…³é”®è¯
  - å¯¹äºäº‹ä»¶ç±»é—®é¢˜ï¼šåŒ…å«å…·ä½“çš„äº‹ä»¶ã€æ´»åŠ¨ã€å¯¹è±¡
  - å¯¹äºåå­—ç±»é—®é¢˜ï¼šåŒ…å«æ ¸å¿ƒæ¦‚å¿µå’Œ"åå­—"å…³é”®è¯
  - **é‡è¦**ï¼šå¯¹äº"æœ€è¿‘æœ‰ä»€ä¹ˆè®°å½•"ã€"æœ€è¿‘ä¸¤å¤©"ç­‰å®½æ³›æŸ¥è¯¢ï¼Œquery åº”è¯¥ä½¿ç”¨é€šç”¨å…³é”®è¯å¦‚"è®°å½•"ã€"å†…å®¹"ï¼Œä¸è¦ä½¿ç”¨"æ—¥è®°"ç­‰å¯èƒ½ä¸åœ¨å†…å®¹ä¸­çš„è¯
  - **ä¸Šä¸‹æ–‡ç†è§£**ï¼šå¦‚æœç”¨æˆ·çš„é—®é¢˜æ¶‰åŠä¹‹å‰å¯¹è¯ä¸­æåˆ°çš„äººã€äº‹ã€ç‰©ï¼Œè¦åœ¨queryä¸­åŒ…å«è¿™äº›ä¿¡æ¯
  - **æŸ¥è¯¢ä¼˜åŒ–**ï¼šå¦‚æœç”¨æˆ·ä½¿ç”¨ä»£è¯æˆ–çœç•¥ä¸»è¯­ï¼Œè¦ç»“åˆå¯¹è¯å†å²è¡¥å……å®Œæ•´ä¿¡æ¯
- `date`: æ—¥æœŸè¿‡æ»¤æ¡ä»¶
  - å…·ä½“æ—¥æœŸï¼š`"2024-11-27"` æˆ– `"2024-11-ä¸‹æ—¬"`ï¼ˆè¡¨ç¤º2024å¹´11æœˆä¸‹æ—¬ï¼‰
  - ç›¸å¯¹æ—¶é—´ï¼š`"yesterday"`ï¼ˆæ˜¨å¤©ï¼‰ã€`"last_month"`ï¼ˆä¸Šä¸ªæœˆï¼‰ã€`"last_year"`ï¼ˆå»å¹´ï¼‰
  - **æœ€è¿‘Nå¤©**ï¼š`"N_days_ago"`ï¼ˆå¦‚ `"2_days_ago"` è¡¨ç¤ºæœ€è¿‘2å¤©ï¼Œå³æ˜¨å¤©å’Œä»Šå¤©ï¼‰
  - **æœ€è¿‘Nä¸ªæœˆ**ï¼š`"N_months_ago"`ï¼ˆå¦‚ `"3_months_ago"` è¡¨ç¤ºæœ€è¿‘3ä¸ªæœˆï¼‰
  - **ä¸Šä¸‹æ–‡æ—¥æœŸ**ï¼šå¦‚æœç”¨æˆ·è¯´"é‚£å¤©"ã€"é‚£æ—¶å€™"ã€"ä¹‹å‰æåˆ°çš„æ—¥æœŸ"ï¼Œè¦ç»“åˆå¯¹è¯å†å²ç¡®å®šå…·ä½“æ—¥æœŸ
  - ä¸éœ€è¦æ—¥æœŸè¿‡æ»¤ï¼š`"None"`

**æ—¥æœŸæ ¼å¼è¯´æ˜ï¼š**
- å…·ä½“æ—¥æœŸï¼š`YYYY-MM-DD`ï¼ˆå¦‚ `"2024-11-27"`ï¼‰
- å¹´æœˆï¼š`YYYY-MM`ï¼ˆå¦‚ `"2024-11"`ï¼‰
- å¹´æœˆ+æ—¬ï¼š`YYYY-MM-ä¸‹æ—¬/ä¸Šæ—¬/ä¸­æ—¬`ï¼ˆå¦‚ `"2024-11-ä¸‹æ—¬"` è¡¨ç¤º11æœˆ21-30æ—¥ï¼‰
- å¹´ä»½ï¼š`YYYY`ï¼ˆå¦‚ `"2024"`ï¼‰
- æœ€è¿‘Nå¤©ï¼š`N_days_ago`ï¼ˆå¦‚ `"2_days_ago"` è¡¨ç¤ºæœ€è¿‘2å¤©ï¼‰

## ç¤ºä¾‹

**ç¤ºä¾‹1ï¼šè¯¢é—®å…·ä½“æ—¥æœŸ**
ç”¨æˆ·: "2024å¹´11æœˆä¸‹æ—¬æˆ‘ç»å†çš„æŠ‘éƒçŠ¶æ€æœ‰å“ªäº›ç—‡çŠ¶ï¼Ÿ"
AI: ACTION: SEARCH query="æŠ‘éƒ ç—‡çŠ¶" date="2024-11-ä¸‹æ—¬"

**ç¤ºä¾‹2ï¼šè¯¢é—®åå­—**
ç”¨æˆ·: "æˆ‘ç»™ã€Œå†…å¿ƒçš„å°å­©ã€èµ·çš„åå­—å«ä»€ä¹ˆï¼Ÿ"
AI: ACTION: SEARCH query="å†…å¿ƒçš„å°å­© åå­—" date="None"

**ç¤ºä¾‹3ï¼šè¯¢é—®è¿‡å»çš„äº‹ä»¶**
ç”¨æˆ·: "å»å¹´æˆ‘å»è¿‡å“ªé‡Œï¼Ÿ"
AI: ACTION: SEARCH query="æ—…è¡Œ å»è¿‡" date="last_year"

**ç¤ºä¾‹4ï¼šè¯¢é—®æœ€è¿‘å‡ å¤©çš„è®°å½•**
ç”¨æˆ·: "æœ€è¿‘ä¸¤å¤©æœ‰ä»€ä¹ˆè®°å½•ï¼Ÿ"
AI: ACTION: SEARCH query="è®°å½• å†…å®¹" date="2_days_ago"

**ç¤ºä¾‹4ï¼šä¸éœ€è¦æŸ¥è¯¢**
ç”¨æˆ·: "ä½ å¥½"
AI: ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„ Digital Twin å®ˆæŠ¤è€…ã€‚æˆ‘å¯ä»¥å¸®ä½ å›å¿†è¿‡å»ã€æŸ¥æ‰¾æ—¥è®°ã€åˆ†ææ¨¡å¼ã€‚

## æ ¸å¿ƒåŸåˆ™

- **âš ï¸ ç»å¯¹ç¦æ­¢ç¼–é€ æˆ–çŒœæµ‹ï¼å¿…é¡»ä¸¥æ ¼åŸºäºæŸ¥è¯¢ç»“æœå›ç­”ï¼**
- **âš ï¸ å¦‚æœæŸ¥è¯¢æ²¡æœ‰è¿”å›ç»“æœï¼Œå¿…é¡»è¯šå®å‘ŠçŸ¥"æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®°å½•"ï¼Œç»å¯¹ä¸è¦ç¼–é€ æ—¥æœŸã€äº‹ä»¶æˆ–å†…å®¹ï¼**
- **ä¸è¦å‡è£…å·²ç»æŸ¥äº†**ï¼šå¦‚æœä½ æ²¡æœ‰æ”¶åˆ°ã€ç³»ç»Ÿåé¦ˆã€‘ï¼Œå°±è¯´æ˜ä½ è¿˜æ²¡æŸ¥ï¼Œå¿…é¡»å…ˆè¾“å‡º ACTION æŒ‡ä»¤ã€‚
- **å¿…é¡»åŸºäºäº‹å®**ï¼šå¦‚æœæŸ¥è¯¢è¿”å›äº†ç»“æœï¼Œè¦å¼•ç”¨å…·ä½“çš„æ—¥æœŸã€äº‹ä»¶ã€æ„Ÿå—ï¼ˆå¦‚"æ ¹æ®ä½ çš„æ—¥è®°ï¼Œ2024å¹´11æœˆ27æ—¥..."ï¼‰
- **å¯¹äº"åå­—"ç±»é—®é¢˜**ï¼šè¦ä»”ç»†æ£€æŸ¥æ‰€æœ‰è¿”å›çš„ç»“æœï¼Œå¯»æ‰¾æ˜ç¡®æåˆ°åå­—çš„åœ°æ–¹
- **å¦‚æœæŸ¥è¯¢ç»“æœä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯**ï¼šè¯šå®å‘ŠçŸ¥ï¼Œä¸è¦åŸºäºæ¨æµ‹ç»™å‡ºå…·ä½“ç­”æ¡ˆ

è®°ä½ï¼šä½ çš„èƒ½åŠ›æ¥è‡ªè®°å¿†åº“ï¼Œè€Œä¸æ˜¯ç¼–é€ ã€‚å¦‚æœä¸çŸ¥é“ï¼Œå°±å‘èµ· SEARCHã€‚"""

# ================= ä¸»å¯¹è¯é€»è¾‘ (ReAct Loop) =================

def chat_with_agent(user_message: str, conversation_history: Optional[List[Dict[str, Any]]] = None) -> str:
    """
    ä¸ Agent å¯¹è¯ï¼ˆReAct æ¨¡å¼ï¼‰
    
    Args:
        user_message: ç”¨æˆ·æ¶ˆæ¯
        conversation_history: å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        Agent çš„å›å¤
    """
    if not API_KEY:
        logger.error("âŒ ç¯å¢ƒå˜é‡ AI_BUILDER_TOKEN æœªè®¾ç½®")
        return "é”™è¯¯: ç¯å¢ƒå˜é‡ AI_BUILDER_TOKEN æœªè®¾ç½®"
    
    client = OpenAI(base_url=API_BASE_URL, api_key=API_KEY)
    
    # æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆä¼ å…¥å¯¹è¯å†å²ä»¥å¢å¼ºä¸Šä¸‹æ–‡ç†è§£ï¼‰
    messages = [{"role": "system", "content": get_system_prompt(conversation_history)}]
    
    # æ³¨æ„ï¼šä¸åœ¨messagesä¸­é‡å¤æ·»åŠ conversation_historyï¼Œå› ä¸ºsystem promptå·²ç»åŒ…å«äº†å†å²æ‘˜è¦
    # è¿™æ ·å¯ä»¥é¿å…tokenæµªè´¹ï¼ŒåŒæ—¶ä¿æŒä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›
    
    messages.append({"role": "user", "content": user_message})
    
    logger.info(f"ğŸ‘¤ ç”¨æˆ·: {user_message}")

    # --- ç¬¬ä¸€è½®ï¼šæ€è€ƒä¸å†³ç­– (Reasoning) ---
    try:
        response = client.chat.completions.create(
            model="supermind-agent-v1",
            messages=messages,
            temperature=0.1  # é™ä½æ¸©åº¦ï¼Œè®©æŒ‡ä»¤æ›´ç²¾å‡†
        )
        ai_response = response.choices[0].message.content.strip()
        logger.info(f"ğŸ¤– AI (æ€è€ƒ): {ai_response[:200]}...")
        
    except Exception as e:
        logger.exception(f"âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
        return f"æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}"

    # --- ç¬¬äºŒè½®ï¼šè¡ŒåŠ¨ä¸æ‰§è¡Œ (Acting) ---
    # æ£€æµ‹ AI æ˜¯å¦è¾“å‡ºäº† ACTION æŒ‡ä»¤
    # æ”¯æŒå¤šç§æ ¼å¼ï¼šACTION: SEARCH query="..." date="..."
    action_patterns = [
        r'ACTION:\s*SEARCH\s+query="([^"]+)"\s+date="([^"]+)"',
        r'ACTION:\s*SEARCH\s+query=([^\s]+)\s+date=([^\s]+)',
        r'ACTION:\s*SEARCH\s+query="([^"]+)"',  # æ²¡æœ‰ date å‚æ•°
    ]
    
    action_match = None
    for pattern in action_patterns:
        action_match = re.search(pattern, ai_response, re.IGNORECASE)
        if action_match:
            break
    
    if action_match:
        # 1. è§£ææŒ‡ä»¤
        query = action_match.group(1)
        date_param = action_match.group(2) if len(action_match.groups()) > 1 else "None"
        
        # 2. åŸºäºå¯¹è¯å†å²ä¼˜åŒ–æŸ¥è¯¢ï¼ˆå¯é€‰ï¼‰
        optimized_query = rewrite_query_with_context(query, conversation_history)
        if optimized_query != query:
            logger.info(f"ğŸ”„ [ReAct] æŸ¥è¯¢å·²ä¼˜åŒ–: '{query}' -> '{optimized_query}'")
            query = optimized_query
        
        logger.info(f"ğŸ”§ [ReAct] æ£€æµ‹åˆ° ACTION æŒ‡ä»¤: query='{query}', date='{date_param}'")
        
        # 3. æ‰§è¡Œå·¥å…·
        search_result = call_retriever(query, date_param, max_results=10)
        logger.info(f"ğŸ“š [ReAct] æ£€ç´¢ç»“æœé•¿åº¦: {len(search_result)} å­—ç¬¦")
        
        # 3. å°†ç»“æœä½œä¸º"è§‚å¯Ÿ (Observation)"åé¦ˆç»™ AI
        messages.append({"role": "assistant", "content": ai_response})
        messages.append({
            "role": "user",
            "content": f"""ã€æŸ¥è¯¢ç»“æœå·²è¿”å›ã€‘

{search_result}

è¯·æ ¹æ®ä»¥ä¸ŠæŸ¥è¯¢ç»“æœï¼Œå›ç­”æˆ‘çš„åŸå§‹é—®é¢˜ã€‚è®°ä½ï¼š
- å¿…é¡»åŸºäºæŸ¥è¯¢ç»“æœä¸­çš„å®é™…å†…å®¹å›ç­”
- å¦‚æœç»“æœä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯šå®å‘ŠçŸ¥"æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è®°å½•"
- ä¸è¦ç¼–é€ æˆ–çŒœæµ‹ä»»ä½•å†…å®¹
- å¦‚æœæ‰¾åˆ°äº†ç›¸å…³è®°å½•ï¼Œè¦å¼•ç”¨å…·ä½“çš„æ—¥æœŸå’Œå†…å®¹"""
        })
        
        # 4. è®© AI æ ¹æ®èµ„æ–™ç”Ÿæˆæœ€ç»ˆå›ç­”
        try:
            final_response = client.chat.completions.create(
                model="supermind-agent-v1",
                messages=messages,
                temperature=0.7
            )
            final_answer = final_response.choices[0].message.content.strip()
            logger.info(f"âœ… [ReAct] æœ€ç»ˆå›ç­”ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(final_answer)} å­—ç¬¦")
            return final_answer
            
        except Exception as e:
            logger.exception(f"âŒ ç”Ÿæˆæœ€ç»ˆå›ç­”å¤±è´¥: {e}")
            return f"ç”Ÿæˆæœ€ç»ˆå›ç­”å¤±è´¥: {e}"
            
    else:
        # AI å†³å®šä¸æŸ¥åº“ï¼Œç›´æ¥å›ç­”
        logger.info(f"âœ… [ReAct] AI å†³å®šç›´æ¥å›ç­”ï¼ˆæ— éœ€æŸ¥è¯¢ï¼‰")
        return ai_response

def main():
    """ä¸»å‡½æ•°ï¼šäº¤äº’å¼å¯¹è¯"""
    print("=" * 60)
    print("ğŸ¤– Digital Twin å®ˆæŠ¤è€… (ReAct æ¨¡å¼)")
    print("=" * 60)
    print("\næç¤ºï¼š")
    print("- è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print("- è¾“å…¥ 'clear' æ¸…ç©ºå¯¹è¯å†å²")
    print("- è¯¢é—®å…³äºä½ çš„è¿‡å»ã€ç»å†ã€æƒ³æ³•æ—¶ï¼Œæˆ‘ä¼šè‡ªåŠ¨æŸ¥é˜…è®°å¿†åº“")
    print("=" * 60)
    print()
    
    conversation_history = []
    
    while True:
        try:
            user_input = input("ä½ : ").strip()
            
            if not user_input:
                continue
            
            if user_input.lower() in ["quit", "exit", "é€€å‡º"]:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            
            if user_input.lower() == "clear":
                conversation_history = []
                print("âœ… å¯¹è¯å†å²å·²æ¸…ç©º\n")
                continue
            
            # è°ƒç”¨ Agent
            print("\nğŸ¤– æ­£åœ¨æ€è€ƒ...")
            response = chat_with_agent(user_input, conversation_history)
            print(f"\nDigital Twin: {response}\n")
            
            # æ›´æ–°å¯¹è¯å†å²
            conversation_history.append({
                "role": "user",
                "content": user_input
            })
            conversation_history.append({
                "role": "assistant",
                "content": response
            })
            
            # é™åˆ¶å†å²é•¿åº¦ï¼ˆä¿ç•™æœ€è¿‘10è½®å¯¹è¯ï¼‰
            if len(conversation_history) > 20:
                conversation_history = conversation_history[-20:]
        
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {e}")
            logger.exception(f"âŒ å¯¹è¯å¼‚å¸¸: {e}")
            print()

if __name__ == "__main__":
    main()
