#!/usr/bin/env python3
"""
å‘é‡ç´¢å¼•æ„å»ºè„šæœ¬
ä½¿ç”¨ Smart Chunking ç­–ç•¥è§£å†³ "Needle in a Haystack" é—®é¢˜
ä» all_chunks.json æ„å»º FAISS å‘é‡ç´¢å¼•
"""

import json
import os
import time
import sys
from typing import List, Dict, Any
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import faiss
import numpy as np
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# å¯¼å…¥ langchain æ–‡æœ¬åˆ‡åˆ†å™¨
try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    HAS_LANGCHAIN = True
except ImportError:
    try:
        # å…¼å®¹æ—§ç‰ˆæœ¬ langchain
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        HAS_LANGCHAIN = True
    except ImportError:
        HAS_LANGCHAIN = False
        print("âŒ é”™è¯¯: langchain æ–‡æœ¬åˆ‡åˆ†å™¨æœªå®‰è£…")
        print("   è¯·å®‰è£…: pip install langchain-text-splitters")
        print("   æˆ–: pip install langchain")
        sys.exit(1)

# ==================== é…ç½®å¸¸é‡ ====================

# API é…ç½®
API_BASE_URL = "https://space.ai-builders.com/backend/v1"  # æ³¨æ„ï¼šåŒ…å« /v1
EMBEDDING_MODEL = "text-embedding-3-small"
API_KEY_ENV = "AI_BUILDER_TOKEN"

# æ‰¹æ¬¡å¤„ç†é…ç½®
BATCH_SIZE = 20  # æ¯æ‰¹å¤„ç†çš„ chunk æ•°é‡
DELAY_BETWEEN_BATCHES = 1.0  # æ‰¹æ¬¡é—´å»¶æ—¶ï¼ˆç§’ï¼‰
MAX_RETRIES = 3  # API è°ƒç”¨é‡è¯•æ¬¡æ•°

# Smart Chunking é…ç½®ï¼ˆé»„é‡‘åŒºé—´ï¼‰
CHUNK_SIZE = 600  # å­—ç¬¦æ•°ï¼Œè®©è¯­ä¹‰æ›´é›†ä¸­ï¼Œçªå‡ºç»†èŠ‚
CHUNK_OVERLAP = 100  # å­—ç¬¦æ•°ï¼Œä¿ç•™ä¸Šä¸‹æ–‡ï¼Œé˜²æ­¢å¥å­è¢«åˆ‡æ–­
CHUNK_SEPARATORS = ["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""]  # ä¼˜å…ˆæŒ‰æ®µè½å’Œå¥å­åˆ‡åˆ†

# é»˜è®¤æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹äºè„šæœ¬æ‰€åœ¨ç›®å½•ï¼‰
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
DEFAULT_INPUT_FILE = os.path.join(SCRIPT_DIR, "all_chunks.json")
DEFAULT_INDEX_FILE = "my_history.index"
DEFAULT_METADATA_FILE = "chunks_metadata.json"

# ==================== æ ¸å¿ƒç±» ====================

class EmbeddingClient:
    """å°è£… Embeddings API è°ƒç”¨çš„å®¢æˆ·ç«¯ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
    
    def __init__(self, api_key: str, base_url: str = API_BASE_URL):
        self.api_key = api_key
        self.base_url = base_url
        self.session = self._create_session()
    
    def _create_session(self):
        """åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„ requests session"""
        session = requests.Session()
        retry_strategy = Retry(
            total=MAX_RETRIES,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["POST"]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("https://", adapter)
        return session
    
    def get_embeddings(self, texts: List[str], model: str = EMBEDDING_MODEL) -> List[List[float]]:
        """
        æ‰¹é‡è·å– embeddings
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            model: æ¨¡å‹åç§°
            
        Returns:
            embeddings åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå‘é‡
        """
        url = f"{self.base_url}/embeddings"
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "input": texts,
            "model": model
        }
        
        try:
            response = self.session.post(url, json=payload, headers=headers, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            # æŒ‰ index æ’åºç¡®ä¿é¡ºåºä¸è¾“å…¥ä¸€è‡´
            sorted_data = sorted(result["data"], key=lambda x: x["index"])
            embeddings = [item["embedding"] for item in sorted_data]
            return embeddings
        
        except requests.exceptions.RequestException as e:
            print(f"\nâŒ API è°ƒç”¨å¤±è´¥: {e}")
            if hasattr(e, 'response') and e.response is not None:
                try:
                    print(f"å“åº”å†…å®¹: {e.response.text}")
                except:
                    pass
            raise

# ==================== Smart Chunking å¤„ç† ====================

def split_chunk_with_text_splitter(chunk: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    ä½¿ç”¨ RecursiveCharacterTextSplitter å°† chunk åˆ‡åˆ†ä¸ºå¤šä¸ªå° chunk
    
    å®æ–½ Smart Chunking ç­–ç•¥ï¼Œè§£å†³ "Needle in a Haystack" é—®é¢˜ï¼š
    - ä½¿ç”¨ langchain çš„ RecursiveCharacterTextSplitter è¿›è¡Œæ™ºèƒ½åˆ‡åˆ†
    - æ¯ä¸ªå°åˆ‡ç‰‡ç»§æ‰¿çˆ¶æ–‡æ¡£çš„æ‰€æœ‰å…ƒæ•°æ®ï¼ˆsource, date, å…¶ä»–å­—æ®µï¼‰
    - å¤„ç†æ—¥æœŸä¸º null çš„æƒ…å†µï¼ˆä¿ç•™ null å€¼åœ¨å…ƒæ•°æ®ä¸­ï¼‰
    - ç”Ÿæˆå”¯ä¸€çš„ ID: {parent_id}_part_{index}
    
    Args:
        chunk: åŸå§‹chunkå­—å…¸ï¼ŒåŒ…å« 'id', 'content', 'source', 'date' ç­‰å­—æ®µ
        
    Returns:
        åˆ‡åˆ†åçš„chunkåˆ—è¡¨ï¼Œæ¯ä¸ªchunkéƒ½æœ‰å”¯ä¸€çš„IDå’Œç»§æ‰¿çš„å…ƒæ•°æ®
    """
    content = chunk.get('content', '')
    if not content or not content.strip():
        return []
    
    parent_id = chunk.get('id', 'unknown')
    parent_source = chunk.get('source')
    parent_date = chunk.get('date')  # å¯èƒ½æ˜¯ null
    
    # ä½¿ç”¨ RecursiveCharacterTextSplitter è¿›è¡Œæ™ºèƒ½åˆ‡åˆ†
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,           # 600 å­—ç¬¦
        chunk_overlap=CHUNK_OVERLAP,      # 100 å­—ç¬¦
        separators=CHUNK_SEPARATORS,      # æ™ºèƒ½åˆ†éš”ç¬¦
        length_function=len
    )
    
    try:
        sub_chunks = splitter.split_text(content)
    except Exception as e:
        print(f"   âš ï¸  åˆ‡åˆ†å¤±è´¥ (ID: {parent_id}): {e}")
        return [chunk]  # å¦‚æœåˆ‡åˆ†å¤±è´¥ï¼Œè¿”å›åŸchunk
    
    # å¦‚æœåªæœ‰ä¸€ä¸ªchunkï¼ˆä¸éœ€è¦åˆ‡åˆ†ï¼‰ï¼Œç›´æ¥è¿”å›åŸchunk
    if len(sub_chunks) <= 1:
        return [chunk]
    
    # ä¸ºæ¯ä¸ª sub_chunk åˆ›å»ºæ–°çš„ chunk å¯¹è±¡
    split_chunks = []
    for index, sub_content in enumerate(sub_chunks):
        if not sub_content.strip():
            continue
        
        # åˆ›å»ºæ–°çš„ chunkï¼Œå®Œæ•´ç»§æ‰¿çˆ¶æ–‡æ¡£çš„å…ƒæ•°æ®
        new_chunk = {
            'id': f"{parent_id}_part_{index}",      # å”¯ä¸€IDæ ¼å¼: {parent_id}_part_{index}
            'content': sub_content.strip(),
            'source': parent_source,                # ç»§æ‰¿ source
            'date': parent_date,                     # ç»§æ‰¿ dateï¼ˆå¯ä»¥æ˜¯ nullï¼‰
        }
        
        # ä¿ç•™å…¶ä»–å…ƒæ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
        for key, value in chunk.items():
            if key not in ['id', 'content', 'source', 'date']:
                new_chunk[key] = value
        
        # æ·»åŠ åˆ‡åˆ†ä¿¡æ¯ï¼ˆç”¨äºè¿½è¸ªå’Œè°ƒè¯•ï¼‰
        new_chunk['_original_id'] = parent_id       # çˆ¶æ–‡æ¡£ID
        new_chunk['_split_index'] = index           # åˆ‡åˆ†ç´¢å¼•ï¼ˆä»0å¼€å§‹ï¼‰
        new_chunk['_total_splits'] = len(sub_chunks)  # æ€»åˆ‡åˆ†æ•°
        
        split_chunks.append(new_chunk)
    
    return split_chunks


def load_and_split_chunks(file_path: str) -> List[Dict[str, Any]]:
    """
    åŠ è½½ chunks JSON æ–‡ä»¶å¹¶åº”ç”¨ Smart Chunking ç­–ç•¥
    
    å¯¹æ‰€æœ‰æ–‡æ¡£è¿›è¡Œæ™ºèƒ½å°åˆ‡ç‰‡å¤„ç†ï¼Œè§£å†³ "Needle in a Haystack" é—®é¢˜ã€‚
    é€šè¿‡å°†å¤§æ–‡æ¡£åˆ‡åˆ†ä¸º600å­—ç¬¦çš„å°åˆ‡ç‰‡ï¼Œç¡®ä¿åƒ"å°ä¸œä¸œ"è¿™æ ·çš„å…³é”®ç»†èŠ‚ä¸ä¼šè¢«ç¨€é‡Šã€‚
    
    å¤„ç†é€»è¾‘ï¼š
    1. åŠ è½½åŸå§‹æ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰
    2. è¿‡æ»¤æ‰ content ä¸ºç©ºçš„è®°å½•ï¼ˆä½†ä¿ç•™æ—¥æœŸä¸º null çš„è®°å½•ï¼‰
    3. å¯¹æ¯ä¸ªæ–‡æ¡£çš„ content ä½¿ç”¨ RecursiveCharacterTextSplitter è¿›è¡Œ Smart Chunking
    4. ä¸ºæ¯ä¸ª sub_chunk åˆ›å»ºæ–°æ–‡æ¡£ï¼Œç»§æ‰¿æ‰€æœ‰å…ƒæ•°æ®ï¼ˆsource, date, å…¶ä»–å­—æ®µï¼‰
    5. å¤„ç†æ—¥æœŸä¸º null çš„æƒ…å†µï¼ˆåœ¨å…ƒæ•°æ®ä¸­ä¿ç•™ null å€¼ï¼‰
    6. ä¸ºæ¯ä¸ª sub_chunk ç”Ÿæˆå”¯ä¸€ ID: {parent_id}_part_{index}
    
    Args:
        file_path: è¾“å…¥çš„ JSON æ–‡ä»¶è·¯å¾„
        
    Returns:
        åˆ‡åˆ†åçš„ chunk åˆ—è¡¨ï¼Œæ¯ä¸ª chunk éƒ½æœ‰å”¯ä¸€çš„ ID å’Œå®Œæ•´çš„å…ƒæ•°æ®
    """
    print(f"ğŸ“– æ­£åœ¨è¯»å–æ–‡ä»¶: {file_path}")
    
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        chunks = json.load(f)
    
    # è¿‡æ»¤æ‰contentä¸ºç©ºçš„è®°å½•ï¼ˆä½†ä¿ç•™æ—¥æœŸä¸ºnullçš„è®°å½•ï¼‰
    valid_chunks = [chunk for chunk in chunks if chunk.get('content') and chunk['content'].strip()]
    skipped = len(chunks) - len(valid_chunks)
    
    if skipped > 0:
        print(f"âš ï¸  è·³è¿‡äº† {skipped} æ¡ content ä¸ºç©ºçš„è®°å½•")
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(valid_chunks)} æ¡æœ‰æ•ˆè®°å½•")
    
    # ç»Ÿè®¡æ—¥æœŸä¸ºnullçš„è®°å½•æ•°
    null_date_count = sum(1 for chunk in valid_chunks if chunk.get('date') is None)
    if null_date_count > 0:
        print(f"ğŸ“… å‘ç° {null_date_count} æ¡è®°å½•çš„æ—¥æœŸä¸º nullï¼ˆå°†ä¿ç•™åœ¨å…ƒæ•°æ®ä¸­ï¼‰")
    
    # åº”ç”¨ Smart Chunking ç­–ç•¥
    print(f"\nğŸ”ª åº”ç”¨ Smart Chunking ç­–ç•¥...")
    print(f"   é…ç½®: chunk_size={CHUNK_SIZE}, chunk_overlap={CHUNK_OVERLAP}")
    print(f"   åˆ†éš”ç¬¦: {CHUNK_SEPARATORS}")
    print(f"   ä½¿ç”¨: RecursiveCharacterTextSplitter (langchain)")
    
    processed_chunks = []
    total_original = len(valid_chunks)
    
    for i, chunk in enumerate(valid_chunks):
        # æ˜¾ç¤ºè¿›åº¦
        if (i + 1) % 100 == 0 or i == 0:
            progress = ((i + 1) * 100 // total_original) if total_original > 0 else 0
            print(f"   å¤„ç†è¿›åº¦: {i+1}/{total_original} ({progress}%)")
        
        # ä½¿ç”¨æ–‡æœ¬åˆ‡åˆ†å™¨åˆ‡åˆ†
        split_chunks = split_chunk_with_text_splitter(chunk)
        processed_chunks.extend(split_chunks)
    
    print(f"\nâœ… Smart Chunking å®Œæˆï¼")
    print(f"   åŸå§‹è®°å½•æ•°: {total_original}")
    print(f"   åˆ‡åˆ†åè®°å½•æ•°: {len(processed_chunks)}")
    if total_original > 0:
        avg_splits = len(processed_chunks) / total_original
        print(f"   å¹³å‡æ¯ä¸ªæ–‡æ¡£åˆ‡åˆ†ä¸º: {avg_splits:.2f} ä¸ªchunk")
    
    # éªŒè¯ï¼šç¡®ä¿æ‰€æœ‰chunkéƒ½æœ‰å”¯ä¸€ID
    ids = [chunk.get('id') for chunk in processed_chunks]
    unique_ids = set(ids)
    if len(ids) != len(unique_ids):
        print(f"âš ï¸  è­¦å‘Š: å‘ç°é‡å¤IDï¼æ€»IDæ•°: {len(ids)}, å”¯ä¸€IDæ•°: {len(unique_ids)}")
    else:
        print(f"âœ… æ‰€æœ‰ {len(unique_ids)} ä¸ªchunkéƒ½æœ‰å”¯ä¸€ID")
    
    return processed_chunks

# ==================== å‘é‡ç”Ÿæˆä¸ç´¢å¼•æ„å»º ====================

def process_batches(chunks: List[Dict[str, Any]], client: EmbeddingClient) -> tuple:
    """
    æ‰¹é‡å¤„ç†chunksï¼Œç”Ÿæˆembeddings
    
    Args:
        chunks: åˆ‡åˆ†åçš„chunkåˆ—è¡¨
        client: EmbeddingClient å®ä¾‹
        
    Returns:
        (embeddingsåˆ—è¡¨, metadataåˆ—è¡¨) - é¡ºåºä¸€ä¸€å¯¹åº”
    """
    all_embeddings = []
    all_metadata = []
    
    total_chunks = len(chunks)
    total_batches = (total_chunks + BATCH_SIZE - 1) // BATCH_SIZE
    
    print(f"\nğŸš€ å¼€å§‹ç”Ÿæˆ Embeddings...")
    print(f"   æ€»chunkæ•°: {total_chunks}")
    print(f"   æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"   æ€»æ‰¹æ¬¡æ•°: {total_batches}")
    print(f"   æ‰¹æ¬¡é—´å»¶æ—¶: {DELAY_BETWEEN_BATCHES}ç§’\n")
    
    for batch_idx in range(total_batches):
        start_idx = batch_idx * BATCH_SIZE
        end_idx = min(start_idx + BATCH_SIZE, total_chunks)
        batch_chunks = chunks[start_idx:end_idx]
        batch_texts = [chunk['content'] for chunk in batch_chunks]
        
        try:
            print(f"ğŸ“¦ æ‰¹æ¬¡ {batch_idx + 1}/{total_batches}: å¤„ç†ç¬¬ {start_idx + 1}-{end_idx} æ¡ ({len(batch_chunks)}æ¡)...", end=" ")
            
            # è°ƒç”¨ API ç”Ÿæˆ embeddings
            batch_embeddings = client.get_embeddings(batch_texts)
            
            # éªŒè¯è¿”å›çš„embeddingsæ•°é‡
            if len(batch_embeddings) != len(batch_chunks):
                raise ValueError(f"è¿”å›çš„embeddingsæ•°é‡ ({len(batch_embeddings)}) ä¸chunkæ•°é‡ ({len(batch_chunks)}) ä¸åŒ¹é…")
            
            # æ·»åŠ åˆ°æ€»åˆ—è¡¨
            all_embeddings.extend(batch_embeddings)
            all_metadata.extend(batch_chunks)
            
            print("âœ…")
            
            # æ‰¹æ¬¡é—´å»¶æ—¶ï¼ˆæœ€åä¸€æ‰¹ä¸éœ€è¦å»¶æ—¶ï¼‰
            if batch_idx < total_batches - 1:
                time.sleep(DELAY_BETWEEN_BATCHES)
        
        except Exception as e:
            print(f"\nâŒ æ‰¹æ¬¡ {batch_idx + 1} å¤„ç†å¤±è´¥: {e}")
            print(f"   è·³è¿‡è¯¥æ‰¹æ¬¡ï¼Œç»§ç»­å¤„ç†...")
            # å¯ä»¥é€‰æ‹©ç»§ç»­æˆ–é€€å‡º
            # è¿™é‡Œé€‰æ‹©ç»§ç»­ï¼Œä½†ä¼šè®°å½•é”™è¯¯
    
    print(f"\nâœ… Embeddings ç”Ÿæˆå®Œæˆï¼")
    print(f"   æˆåŠŸå¤„ç†: {len(all_embeddings)} æ¡è®°å½•")
    
    if len(all_embeddings) != total_chunks:
        print(f"âš ï¸  è­¦å‘Š: æˆåŠŸå¤„ç†çš„è®°å½•æ•° ({len(all_embeddings)}) ä¸æ€»chunkæ•° ({total_chunks}) ä¸ä¸€è‡´")
    
    return all_embeddings, all_metadata


def build_faiss_index(embeddings: List[List[float]]) -> faiss.Index:
    """
    æ„å»ºFAISSç´¢å¼•
    
    Args:
        embeddings: embeddingsåˆ—è¡¨
        
    Returns:
        FAISSç´¢å¼•å¯¹è±¡
    """
    print(f"\nğŸ”¨ æ­£åœ¨æ„å»ºFAISSç´¢å¼•...")
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    embeddings_array = np.array(embeddings, dtype=np.float32)
    dimension = embeddings_array.shape[1]
    vector_count = len(embeddings)
    
    print(f"   å‘é‡ç»´åº¦: {dimension}")
    print(f"   å‘é‡æ•°é‡: {vector_count}")
    
    # åˆ›å»ºIndexFlatL2ç´¢å¼•ï¼ˆL2è·ç¦»ï¼‰
    index = faiss.IndexFlatL2(dimension)
    
    # æ·»åŠ å‘é‡åˆ°ç´¢å¼•
    index.add(embeddings_array)
    
    print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼")
    return index


def save_index_and_metadata(index: faiss.Index, metadata: List[Dict[str, Any]], 
                            index_path: str, metadata_path: str):
    """
    ä¿å­˜ç´¢å¼•å’Œå…ƒæ•°æ®
    
    Args:
        index: FAISSç´¢å¼•å¯¹è±¡
        metadata: å…ƒæ•°æ®åˆ—è¡¨ï¼ˆé¡ºåºå¿…é¡»ä¸ç´¢å¼•ä¸€ä¸€å¯¹åº”ï¼‰
        index_path: ç´¢å¼•æ–‡ä»¶ä¿å­˜è·¯å¾„
        metadata_path: å…ƒæ•°æ®æ–‡ä»¶ä¿å­˜è·¯å¾„
    """
    print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜æ–‡ä»¶...")
    
    # éªŒè¯æ•°é‡ä¸€è‡´æ€§
    if index.ntotal != len(metadata):
        print(f"âš ï¸  è­¦å‘Š: ç´¢å¼•å‘é‡æ•° ({index.ntotal}) ä¸å…ƒæ•°æ®æ•° ({len(metadata)}) ä¸ä¸€è‡´")
    
    # ä¿å­˜FAISSç´¢å¼•
    faiss.write_index(index, index_path)
    print(f"   âœ… ç´¢å¼•å·²ä¿å­˜: {index_path}")
    
    # ä¿å­˜å…ƒæ•°æ®ï¼ˆä¿æŒJSONæ ¼å¼ï¼Œnullå€¼ä¼šè¢«æ­£ç¡®ä¿å­˜ï¼‰
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    print(f"   âœ… å…ƒæ•°æ®å·²ä¿å­˜: {metadata_path}")
    
    print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼")
    print(f"   ç´¢å¼•æ–‡ä»¶: {index_path}")
    print(f"   å…ƒæ•°æ®æ–‡ä»¶: {metadata_path}")
    print(f"   æ€»è®°å½•æ•°: {len(metadata)}")

# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description='ä½¿ç”¨ Smart Chunking ç­–ç•¥æ„å»ºå‘é‡ç´¢å¼•',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python indexer.py
  python indexer.py --input custom_input.json --output-index custom.index
        """
    )
    parser.add_argument('--input', '-i', 
                       default=DEFAULT_INPUT_FILE,
                       help=f'è¾“å…¥JSONæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: {DEFAULT_INPUT_FILE}ï¼‰')
    parser.add_argument('--output-index', '-o',
                       default=DEFAULT_INDEX_FILE,
                       help=f'è¾“å‡ºç´¢å¼•æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: {DEFAULT_INDEX_FILE}ï¼‰')
    parser.add_argument('--output-metadata', '-m',
                       default=DEFAULT_METADATA_FILE,
                       help=f'è¾“å‡ºå…ƒæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: {DEFAULT_METADATA_FILE}ï¼‰')
    args = parser.parse_args()
    
    # æ£€æŸ¥API key
    api_key = os.getenv(API_KEY_ENV)
    if not api_key:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°ç¯å¢ƒå˜é‡ AI_BUILDER_TOKEN")
        print("\nè¯·è®¾ç½®API key:")
        print("   export AI_BUILDER_TOKEN='your_api_key_here'")
        print("\næˆ–åˆ›å»º .env æ–‡ä»¶:")
        print("   echo 'AI_BUILDER_TOKEN=your_api_key_here' > .env")
        sys.exit(1)
    
    # è¾“å…¥æ–‡ä»¶è·¯å¾„
    input_file = args.input
    
    if not os.path.exists(input_file):
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        sys.exit(1)
    
    print("=" * 60)
    print("ğŸš€ å‘é‡ç´¢å¼•æ„å»ºå·¥å…· (Smart Chunking)")
    print("=" * 60)
    print(f"è¾“å…¥æ–‡ä»¶: {input_file}")
    print(f"è¾“å‡ºç´¢å¼•: {args.output_index}")
    print(f"è¾“å‡ºå…ƒæ•°æ®: {args.output_metadata}")
    print("=" * 60)
    
    try:
        # 1. åŠ è½½chunkså¹¶åº”ç”¨Smart Chunking
        chunks = load_and_split_chunks(input_file)
        
        if not chunks:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„chunkså¯å¤„ç†")
            sys.exit(1)
        
        # 2. åˆ›å»ºAPIå®¢æˆ·ç«¯
        client = EmbeddingClient(api_key)
        
        # 3. æ‰¹é‡å¤„ç†ç”Ÿæˆembeddings
        embeddings, metadata = process_batches(chunks, client)
        
        if not embeddings:
            print("âŒ æ²¡æœ‰æˆåŠŸç”Ÿæˆä»»ä½•embeddings")
            sys.exit(1)
        
        # éªŒè¯æ•°é‡ä¸€è‡´æ€§
        if len(embeddings) != len(metadata):
            print(f"âŒ é”™è¯¯: embeddingsæ•°é‡ ({len(embeddings)}) ä¸metadataæ•°é‡ ({len(metadata)}) ä¸ä¸€è‡´")
            sys.exit(1)
        
        # 4. æ„å»ºFAISSç´¢å¼•
        index = build_faiss_index(embeddings)
        
        # 5. ä¿å­˜ç´¢å¼•å’Œå…ƒæ•°æ®
        save_index_and_metadata(index, metadata, 
                               index_path=args.output_index,
                               metadata_path=args.output_metadata)
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
