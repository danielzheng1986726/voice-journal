#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢é‡ç´¢å¼•æ„å»ºè„šæœ¬
åªå¯¹æ–°è®°å½•å»ºç«‹ç´¢å¼•ï¼Œè¿½åŠ åˆ°ç°æœ‰ç´¢å¼•ä¸­
"""

import json
import os
import sys
import time
from pathlib import Path
from typing import List, Dict, Any
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import faiss
import numpy as np
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®
# ä½¿ç”¨è„šæœ¬æ‰€åœ¨ç›®å½•ä½œä¸º vector_indexer ç›®å½•
VECTOR_INDEXER_DIR = Path(__file__).parent
ALL_CHUNKS_FILE = VECTOR_INDEXER_DIR / "all_chunks.json"
INDEX_FILE = VECTOR_INDEXER_DIR / "my_history.index"
METADATA_FILE = VECTOR_INDEXER_DIR / "chunks_metadata.json"
INDEXED_IDS_FILE = VECTOR_INDEXER_DIR / ".indexed_ids.json"  # è®°å½•å·²ç´¢å¼•çš„IDåˆ—è¡¨

API_BASE_URL = "https://space.ai-builders.com/backend/v1"
EMBEDDING_MODEL = "text-embedding-3-small"
API_KEY = os.getenv("AI_BUILDER_TOKEN")

if not API_KEY:
    print("âŒ é”™è¯¯: AI_BUILDER_TOKEN ç¯å¢ƒå˜é‡æœªè®¾ç½®")
    sys.exit(1)

def get_embedding(text: str) -> List[float]:
    """è·å–æ–‡æœ¬çš„å‘é‡è¡¨ç¤º"""
    url = f"{API_BASE_URL}/embeddings"
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "input": [text],
        "model": EMBEDDING_MODEL
    }
    
    session = requests.Session()
    retry_strategy = Retry(
        total=3,
        backoff_factor=1,
        status_forcelist=[429, 500, 502, 503, 504]
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("https://", adapter)
    
    try:
        response = session.post(url, json=payload, headers=headers, timeout=60)
        response.raise_for_status()
        result = response.json()
        return result["data"][0]["embedding"]
    except Exception as e:
        print(f"âŒ è·å–å‘é‡å¤±è´¥: {e}")
        raise

def load_indexed_ids() -> set:
    """åŠ è½½å·²ç´¢å¼•çš„è®°å½•IDåˆ—è¡¨"""
    if INDEXED_IDS_FILE.exists():
        try:
            with open(INDEXED_IDS_FILE, 'r', encoding='utf-8') as f:
                return set(json.load(f))
        except Exception as e:
            print(f"âš ï¸  è¯»å–å·²ç´¢å¼•IDåˆ—è¡¨å¤±è´¥: {e}ï¼Œå°†é‡æ–°ç´¢å¼•")
            return set()
    return set()

def save_indexed_ids(indexed_ids: set):
    """ä¿å­˜å·²ç´¢å¼•çš„è®°å½•IDåˆ—è¡¨"""
    with open(INDEXED_IDS_FILE, 'w', encoding='utf-8') as f:
        json.dump(list(indexed_ids), f, ensure_ascii=False, indent=2)

def incremental_index():
    """å¢é‡ç´¢å¼•ï¼šåªå¤„ç†æ–°è®°å½•"""
    
    # 1. åŠ è½½æ•°æ®
    if not ALL_CHUNKS_FILE.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {ALL_CHUNKS_FILE}")
        return False
    
    with open(ALL_CHUNKS_FILE, 'r', encoding='utf-8') as f:
        all_chunks = json.load(f)
    
    indexed_ids = load_indexed_ids()
    
    # 2. æ‰¾å‡ºæ–°è®°å½•
    new_chunks = [chunk for chunk in all_chunks if chunk.get('id') not in indexed_ids]
    
    if not new_chunks:
        print("âœ… æ²¡æœ‰æ–°è®°å½•éœ€è¦ç´¢å¼•")
        return True
    
    print(f"ğŸ“Š å‘ç° {len(new_chunks)} æ¡æ–°è®°å½•éœ€è¦ç´¢å¼•ï¼ˆæ€»å…± {len(all_chunks)} æ¡ï¼‰")
    
    # 3. åŠ è½½ç°æœ‰ç´¢å¼•å’Œå…ƒæ•°æ®
    if INDEX_FILE.exists() and METADATA_FILE.exists():
        try:
            index = faiss.read_index(str(INDEX_FILE))
            with open(METADATA_FILE, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            print(f"âœ… åŠ è½½ç°æœ‰ç´¢å¼•: {index.ntotal} æ¡å‘é‡")
        except Exception as e:
            print(f"âš ï¸  åŠ è½½ç°æœ‰ç´¢å¼•å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°ç´¢å¼•")
            index = None
            metadata = []
    else:
        print("ğŸ“ ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°ç´¢å¼•")
        index = None
        metadata = []
    
    # 4. ä¸ºæ–°è®°å½•ç”Ÿæˆå‘é‡å¹¶è¿½åŠ 
    new_vectors = []
    new_metadata = []
    
    print(f"ğŸ”„ å¼€å§‹å¤„ç† {len(new_chunks)} æ¡æ–°è®°å½•...")
    for i, chunk in enumerate(new_chunks, 1):
        try:
            content = chunk.get('content', '')
            if not content:
                print(f"âš ï¸  è·³è¿‡ç©ºå†…å®¹è®°å½•: {chunk.get('id')}")
                continue
            
            # ç”Ÿæˆå‘é‡
            print(f"  [{i}/{len(new_chunks)}] å¤„ç†: {chunk.get('id')}...", end=' ', flush=True)
            embedding = get_embedding(content)
            new_vectors.append(embedding)
            new_metadata.append(chunk)
            indexed_ids.add(chunk.get('id'))
            print("âœ“")
            
            # é¿å…APIé™æµ
            if i % 10 == 0:
                time.sleep(0.5)
                
        except Exception as e:
            print(f"âœ— é”™è¯¯: {e}")
            continue
    
    if not new_vectors:
        print("âš ï¸  æ²¡æœ‰æˆåŠŸç”Ÿæˆä»»ä½•å‘é‡")
        return False
    
    # 5. è¿½åŠ åˆ°ç´¢å¼•
    if index is None:
        # åˆ›å»ºæ–°ç´¢å¼•
        dimension = len(new_vectors[0])
        index = faiss.IndexFlatL2(dimension)
        print(f"ğŸ“ åˆ›å»ºæ–°ç´¢å¼•ï¼Œç»´åº¦: {dimension}")
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶è¿½åŠ 
    vectors_array = np.array(new_vectors, dtype=np.float32)
    index.add(vectors_array)
    
    # æ›´æ–°å…ƒæ•°æ®
    metadata.extend(new_metadata)
    
    # 6. ä¿å­˜ç´¢å¼•å’Œå…ƒæ•°æ®
    faiss.write_index(index, str(INDEX_FILE))
    with open(METADATA_FILE, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜å·²ç´¢å¼•IDåˆ—è¡¨
    save_indexed_ids(indexed_ids)
    
    print(f"âœ… å¢é‡ç´¢å¼•å®Œæˆï¼")
    print(f"   - æ–°å¢: {len(new_vectors)} æ¡")
    print(f"   - æ€»è®¡: {index.ntotal} æ¡å‘é‡")
    print(f"   - å…ƒæ•°æ®: {len(metadata)} æ¡")
    
    return True

if __name__ == "__main__":
    try:
        success = incremental_index()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
